{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed16f904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zachary Katz\n",
    "# zachary_katz@mines.edu\n",
    "# November 2024\n",
    "\n",
    "\"\"\"\n",
    "Plot Figures for Whillans AGU POSTER 2024\n",
    "\n",
    "\n",
    "v1.0: 22 November 2024\n",
    "    Poster figs\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Imports\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "\n",
    "from astropy.timeseries import LombScargle\n",
    "\n",
    "from pyproj import CRS, Transformer\n",
    "import shapefile\n",
    "import shapely\n",
    "from shapely.plotting import plot_line\n",
    "from shapely.geometry import LineString\n",
    "\n",
    "from cmap import Colormap\n",
    "import xarray as xr\n",
    "import rasterio\n",
    "\n",
    "\n",
    "sys.path.insert(\n",
    "    0,\n",
    "    \"/mnt/c/Users/ZacharyKatz/Desktop/WhillansCatPaper/WhillansCatalogPaper/src/Tides\",\n",
    ")\n",
    "import Tides\n",
    "\n",
    "################################################################################\n",
    "########################## User Defined Variables ##############################\n",
    "\n",
    "min_stas = 2  # Which catalog to use\n",
    "tide_dir = \"/mnt/c/Users/ZacharyKatz/Desktop/Research/Background\"\n",
    "tide_mod = \"CATS2008-v2023\"\n",
    "\n",
    "########################## User Defined Variables ##############################\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1851bfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load slip times based on second derivative\n",
    "ev_data = {\"event\": [], \"trace_time\": []}\n",
    "df = pd.read_csv(f\"../AllEventStartTimes_{min_stas}stas.txt\", sep=\"\\t\")\n",
    "ev_data[\"ev_time\"] = df[\"EventStartTime\"]\n",
    "\n",
    "# Load no data from txt file\n",
    "no_data = {\"interval\": [], \"starts\": [], \"ends\": []}\n",
    "df = pd.read_csv(f\"../no_data_{min_stas}stas.txt\", sep=\"\\t\")\n",
    "no_data[\"starts\"] = df[\"start\"]\n",
    "no_data[\"ends\"] = df[\"end\"]\n",
    "diff = [\n",
    "    datetime.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\")\n",
    "    - datetime.datetime.strptime(y, \"%Y-%m-%d %H:%M:%S\")\n",
    "    for x, y in zip(no_data[\"ends\"], no_data[\"starts\"])\n",
    "]\n",
    "no_data[\"interval\"] = diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3eea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate time since last events\n",
    "time_since_last_ev = [\n",
    "    datetime.timedelta(seconds=1000000)\n",
    "]  # Set large for first event because prior event time unknown\n",
    "for i, event in enumerate(ev_data[\"ev_time\"]):\n",
    "    if i > 0:\n",
    "        diff = datetime.datetime.strptime(\n",
    "            ev_data[\"ev_time\"][i], \"%Y-%m-%d %H:%M:%S\"\n",
    "        ) - datetime.datetime.strptime(ev_data[\"ev_time\"][i - 1], \"%Y-%m-%d %H:%M:%S\")\n",
    "        time_since_last_ev.append(diff)\n",
    "ev_data[\"time_since_last_ev\"] = time_since_last_ev\n",
    "print([(a.days * 24 * 3600 + a.seconds) / 3600 for a in ev_data[\"time_since_last_ev\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518e7331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate difference in tides across basin at event times (from icesat2/gztides.ipynb)\n",
    "# Helper functions\n",
    "def inBox(points, bbox):\n",
    "    # Return points from points that are in bbox\n",
    "    inbox = []\n",
    "    for point in points:\n",
    "        x = point[0]\n",
    "        y = point[1]\n",
    "        if x > bbox[0] and x < bbox[2] and y > bbox[1] and y < bbox[3]:\n",
    "            inbox.append(point)\n",
    "    return inbox\n",
    "\n",
    "\n",
    "def interpolateLine(line, dist):\n",
    "    # Return a new list of points from a linestring that has been interpolated at\n",
    "    # distance dist. Line must be linestring, dist in m for our case\n",
    "\n",
    "    # Follows example from:\n",
    "    # https://stackoverflow.com/questions/34906124/interpolating-every-x-distance-along-multiline-in-shapely\n",
    "    length = shapely.length(line)\n",
    "    num_vert = int(np.round(length / dist))\n",
    "    # print(num_vert)\n",
    "    return [line.interpolate(n * dist) for n in range(num_vert)]\n",
    "\n",
    "\n",
    "def xy2ll(x, y):\n",
    "    \"\"\"\n",
    "     Transform coordinates to geodetic coordinates (lon, lat)\n",
    "     from Antarctic Polar Stereograph coordinates (x, y)\n",
    "\n",
    "    lon,lat = xy2ll(x,y)\n",
    "    \"\"\"\n",
    "    crs_ll = CRS(\"EPSG:4326\")\n",
    "    crs_xy = CRS(\"EPSG:3031\")\n",
    "    xy_to_ll = Transformer.from_crs(crs_xy, crs_ll, always_xy=True)\n",
    "    lon, lat = xy_to_ll.transform(x, y)\n",
    "    return lon, lat\n",
    "\n",
    "\n",
    "def ll2xy(lon, lat):\n",
    "    \"\"\"\n",
    "    Transform coordinates from input geodetic coordinates (lon, lat)\n",
    "    to output Antarctic Polar Stereographic coordinates (x, y)\n",
    "    Can also take lists of floats!\n",
    "\n",
    "    Parameters\n",
    "    lon - Geodetic longitude in EPSG:4326 [float]\n",
    "    lat - Geodetic latitude in EPSG:4326 [float]\n",
    "\n",
    "    Returns\n",
    "    x - Antarctic Polar Stereographic (EPSG:3031) x [float]\n",
    "    y - Antarctic Polar Stereographic (EPSG:3031) y [float]\n",
    "    \"\"\"\n",
    "\n",
    "    crs_ll = CRS(\"EPSG:4326\")\n",
    "    crs_xy = CRS(\"EPSG:3031\")\n",
    "    ll_to_xy = Transformer.from_crs(crs_ll, crs_xy, always_xy=True)\n",
    "    x, y = ll_to_xy.transform(lon, lat)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def dist_sq(x, y):\n",
    "    # Returns min dist squared between two points\n",
    "    delta_x = y[0] - x[0]\n",
    "    delta_y = y[1] - x[1]\n",
    "    return delta_x * delta_x + delta_y * delta_y\n",
    "\n",
    "\n",
    "# Extract coordinates of grounding line\n",
    "\n",
    "# Tuning parameters\n",
    "offset = 5000  # Distance from grounding line to offset, m\n",
    "pt_dist = 500  # Distance between tide sampling points, m\n",
    "\n",
    "# Arrays to pass to tides\n",
    "x_tide = []\n",
    "y_tide = []\n",
    "\n",
    "bbox = [-280000, -800000, -60000, -420000]  # Includes all of Crary\n",
    "fig, ax = plt.subplots(\n",
    "    figsize=[(bbox[2] - bbox[0]) / 100000 * 3, (bbox[3] - bbox[1]) / 100000 * 3]\n",
    ")\n",
    "# gl_path = '/mnt/d/Background/Antarctica_masks/scripps_antarctica_polygons_v1.shp'\n",
    "gl_path = \"/mnt/c/Users/ZacharyKatz/Desktop/Research/Background/Antarctica_masksX/scripps_antarctica_polygons_v1.shp\"\n",
    "sf = shapefile.Reader(gl_path)\n",
    "for i, shape in enumerate(sf.shapes(bbox=bbox)):\n",
    "    if i == 3:\n",
    "        points = shape.points\n",
    "        # Only get points within bounding box\n",
    "        inbox = inBox(points, bbox)\n",
    "        line = LineString(inbox)\n",
    "        dilated = line.buffer(3000, single_sided=True)\n",
    "        offset_line = line.offset_curve(offset)\n",
    "        plot_line(line, ax=ax, add_points=False, color=\"black\")\n",
    "        plot_line(offset_line, ax=ax, add_points=False, color=\"gray\")\n",
    "        # plot_polygon(dilated, ax=ax, add_points=False, color='black')\n",
    "\n",
    "        interpolated = interpolateLine(offset_line, pt_dist)\n",
    "        xs = [point.x for point in interpolated]\n",
    "        ys = [point.y for point in interpolated]\n",
    "        ax.scatter(xs, ys, s=1, color=\"red\")\n",
    "        x_tide.append(xs)\n",
    "        y_tide.append(ys)\n",
    "\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(50000))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(50000))\n",
    "ax.set_xlabel(\"X [m]\", size=20)\n",
    "ax.set_ylabel(\"Y [m]\", size=20)\n",
    "ax.tick_params(labelsize=15)\n",
    "\n",
    "# Flatten the list. Not sure why this works but here's the source\n",
    "# https://www.scaler.com/topics/flatten-list-python/\n",
    "x_tide = sum(x_tide, [])\n",
    "y_tide = sum(y_tide, [])\n",
    "\n",
    "lon_lats = [xy2ll(x0, y0) for x0, y0 in zip(x_tide, y_tide)]\n",
    "lons = [x[0] for x in lon_lats]\n",
    "lats = [x[1] for x in lon_lats]\n",
    "print(lon_lats)\n",
    "\n",
    "x_south = lon_lats[900][0]\n",
    "y_south = lon_lats[900][1]\n",
    "x_north = lon_lats[-200][0]\n",
    "y_north = lon_lats[-200][1]\n",
    "x_n, y_n = ll2xy(x_north, y_north)\n",
    "x_s, y_s = ll2xy(x_south, y_south)\n",
    "ax.scatter(x_n, y_n, color=\"blue\", zorder=2)\n",
    "ax.scatter(x_s, y_s, color=\"purple\", zorder=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76f9518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tide locations\n",
    "\n",
    "vel_mag_path = \"/mnt/c/Users/ZacharyKatz/Desktop/Research/Background/antarctic_ice_vel_phase_map_v01-vmag.nc\"\n",
    "moa_path = (\n",
    "    \"/mnt/c/Users/ZacharyKatz/Desktop/Research/Background/moa750_2009_hp1_v1.1.tif\"\n",
    ")\n",
    "vel_mag = xr.open_dataarray(vel_mag_path)\n",
    "bbox = [-300000, -800000, -60000, -420000]  # Includes all of Crary\n",
    "vel_mag = vel_mag.isel(x=slice(4000, 8000), y=slice(6000, 8000))\n",
    "x_min_v, x_max_v = vel_mag[\"x\"].min().item(), vel_mag[\"x\"].max().item()\n",
    "y_min_v, y_max_v = vel_mag[\"y\"].min().item(), vel_mag[\"y\"].max().item()\n",
    "\n",
    "with rasterio.open(moa_path) as moa:\n",
    "    bounds = moa.bounds\n",
    "    left, bottom, right, top = bounds.left, bounds.bottom, bounds.right, bounds.top\n",
    "    moa_dat = moa.read(1)\n",
    "\n",
    "ext = (left, right, bottom, top)\n",
    "\n",
    "\n",
    "oslo = Colormap(\"crameri:oslo\").to_mpl()\n",
    "# Static frame\n",
    "bbox = [-300000, -800000, -60000, -420000]  # Includes all of Crary\n",
    "gl_path = \"/mnt/c/Users/ZacharyKatz/Desktop/Research/Background/Antarctica_masksX/scripps_antarctica_polygons_v1.shp\"\n",
    "sf = shapefile.Reader(gl_path)\n",
    "fig, ax = plt.subplots(\n",
    "    figsize=[(bbox[2] - bbox[0]) / 100000 * 5, (bbox[3] - bbox[1]) / 100000 * 3]\n",
    ")\n",
    "fig.set_tight_layout(True)\n",
    "ax.imshow(moa_dat, extent=ext, cmap=\"gray\", vmin=15000, vmax=17000)\n",
    "v = ax.imshow(\n",
    "    vel_mag,\n",
    "    extent=(x_min_v, x_max_v, y_min_v, y_max_v),\n",
    "    cmap=oslo,\n",
    "    vmax=500,\n",
    "    vmin=0,\n",
    "    alpha=0.7,\n",
    ")\n",
    "for i, shape in enumerate(sf.shapes(bbox=bbox)):\n",
    "    if i > 0:\n",
    "        points = shape.points\n",
    "        # Only get points within bounding box\n",
    "        inbox = inBox(points, bbox)\n",
    "        line = LineString(inbox)\n",
    "        plot_line(line, ax=ax, add_points=False, color=\"white\")\n",
    "x_min, x_max = bbox[0], bbox[2]\n",
    "y_min, y_max = bbox[1], bbox[3]\n",
    "ax.set_xlim(x_min, x_max)\n",
    "ax.set_ylim(y_min, y_max)\n",
    "KM_SCALE = 1e3\n",
    "ticks_x = ticker.FuncFormatter(lambda x, pos: \"{0:g}\".format(x / KM_SCALE))\n",
    "ax.xaxis.set_major_formatter(ticks_x)\n",
    "ticks_y = ticker.FuncFormatter(lambda x, pos: \"{0:g}\".format(x / KM_SCALE))\n",
    "ax.yaxis.set_major_formatter(ticks_y)\n",
    "ax.set_xlabel(\"X (PS71) [km]\", size=20)\n",
    "ax.set_ylabel(\"Y (PS71) [km]\", size=20)\n",
    "ax.tick_params(labelsize=15)\n",
    "ax.tick_params(size=4)\n",
    "# ax.set_facecolor(\"black\")\n",
    "cbarv = fig.colorbar(v)\n",
    "cbarv.ax.tick_params(labelsize=15)\n",
    "cbarv.ax.set_ylabel(\"Ice Velocity [m/a]\", size=20)\n",
    "x, y = ll2xy(lons, lats)\n",
    "places = [\"gz05\", \"North\", \"South\"]\n",
    "ax.scatter(x, y, s=70, color=\"white\", marker=\"v\")\n",
    "print(x)\n",
    "ax.text(x[0] + 2000, y[0], \"gz05\", color=\"white\", size=25)\n",
    "ax.text(x[1] + 2000, y[1], \"North\", color=\"white\", size=25)\n",
    "ax.text(x[2] + 2000, y[2], \"South\", color=\"white\", size=25)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c87114d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tide time series at gz05\n",
    "spacing = 4  # Minutes\n",
    "dates_timeseries = []\n",
    "initial_time = datetime.datetime.strptime(\"2007-12-01 00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "for i in range(24 * 365 * 12 * 15):\n",
    "    dates_timeseries.append(initial_time + datetime.timedelta(minutes=spacing * i))\n",
    "\n",
    "lats = [-84.2986, y_north, y_south]\n",
    "lons = [-164.5206, x_north, x_south]\n",
    "places = [\"gz05\", \"North\", \"South\"]\n",
    "\n",
    "tides = Tides.Tide(tide_mod, tide_dir)\n",
    "for lat, lon, place in zip(lats, lons, places):\n",
    "    ev_data[f\"tides{place}\"] = tides.tidal_elevation(\n",
    "        [lon], [lat], dates_timeseries\n",
    "    ).data.T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489593a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tides at event times\n",
    "times_to_calculate_tide = ev_data[\"ev_time\"]\n",
    "\n",
    "dates = [\n",
    "    datetime.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\") for x in times_to_calculate_tide\n",
    "]\n",
    "\n",
    "lats = [-84.2986, y_north, y_south]\n",
    "lons = [-164.5206, x_north, x_south]\n",
    "places = [\"gz05\", \"North\", \"South\"]\n",
    "\n",
    "for lat, lon, place in zip(lats, lons, places):\n",
    "    print(lat, lon, place)\n",
    "    ev_data[f\"tide_event_time_{place}\"] = tides.tidal_elevation(\n",
    "        [lon], [lat], dates\n",
    "    ).data.T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4020cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tides at event times\n",
    "df = pd.DataFrame(\n",
    "    data={\n",
    "        \"time\": ev_data[\"ev_time\"],\n",
    "        \"tide_event_time\": ev_data[\"tide_event_time_gz05\"],\n",
    "    }\n",
    ")\n",
    "df.to_csv(\"tide_event_time_gz05.txt\", index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "808bd33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distance between event and nearest low and high tides\n",
    "def dists(ev_data, place):\n",
    "    # Calculate tidal maxima and minima for comparison\n",
    "\n",
    "    # https://stackoverflow.com/questions/4624970/finding-local-maxima-minima-with-numpy-in-a-1d-numpy-array\n",
    "\n",
    "    # Get as indices\n",
    "    pks = scipy.signal.find_peaks(ev_data[f\"tides{place}\"], distance=60 / spacing * 3)[\n",
    "        0\n",
    "    ]  # Require 5 hrs between peaks\n",
    "    vlys = scipy.signal.find_peaks(\n",
    "        -1 * ev_data[f\"tides{place}\"], distance=60 / spacing * 3\n",
    "    )[0]  # Require 5 hrs between valleys\n",
    "\n",
    "    # Also save as times\n",
    "    pk_times = [dates_timeseries[pk] for pk in pks]\n",
    "    vly_times = [dates_timeseries[vly] for vly in vlys]\n",
    "\n",
    "    # Find skipped low tide events (No event within +- 7 hrs of vly)\n",
    "    skipped_lows = []\n",
    "    events = [\n",
    "        datetime.datetime.strptime(event, \"%Y-%m-%d %H:%M:%S\")\n",
    "        for event in ev_data[\"ev_time\"]\n",
    "    ]\n",
    "    for i, vly in enumerate(vlys):\n",
    "        valley = dates_timeseries[vly]\n",
    "        closest_event = min(events, key=lambda d: abs(d - valley))\n",
    "        skipped = -1\n",
    "        if abs(closest_event - valley) > datetime.timedelta(hours=6):\n",
    "            skipped = 1\n",
    "        skipped_lows.append(skipped)\n",
    "\n",
    "    skipped_highs = []\n",
    "    events = [\n",
    "        datetime.datetime.strptime(event, \"%Y-%m-%d %H:%M:%S\")\n",
    "        for event in ev_data[\"ev_time\"]\n",
    "    ]\n",
    "    for i, pk in enumerate(pks):\n",
    "        peak = dates_timeseries[pk]\n",
    "        closest_event = min(events, key=lambda d: abs(d - peak))\n",
    "        skipped = -1\n",
    "        if abs(closest_event - peak) > datetime.timedelta(hours=6):\n",
    "            skipped = 1\n",
    "        skipped_highs.append(skipped)\n",
    "\n",
    "    vly_date = [dates_timeseries[vly] for vly in vlys]\n",
    "    vly_tide = [ev_data[f\"tides{place}\"][vly] for vly in vlys]\n",
    "\n",
    "    pk_date = [dates_timeseries[pk] for pk in pks]\n",
    "    pk_tide = [ev_data[f\"tides{place}\"][pk] for pk in pks]\n",
    "\n",
    "    dist_to_highs = []\n",
    "    dist_to_lows = []\n",
    "    closest_highs = []\n",
    "    closest_lows = []\n",
    "    high_closers = []\n",
    "    ht_to_highs = []\n",
    "    ht_to_lows = []\n",
    "    high_ht_closers = []\n",
    "    for i, event in enumerate(ev_data[\"ev_time\"]):\n",
    "        event = datetime.datetime.strptime(event, \"%Y-%m-%d %H:%M:%S\")\n",
    "        closest_high = min(pk_times, key=lambda d: abs(d - event))\n",
    "        dist_to_high = event - closest_high\n",
    "        closest_low = min(vly_times, key=lambda d: abs(d - event))\n",
    "\n",
    "        ht_to_high = (\n",
    "            ev_data[f\"tide_event_time_{place}\"][i]\n",
    "            - ev_data[f\"tides{place}\"][pks[pk_times.index(closest_high)]]\n",
    "        )\n",
    "        ht_to_low = (\n",
    "            ev_data[f\"tide_event_time_{place}\"][i]\n",
    "            - ev_data[f\"tides{place}\"][vlys[vly_times.index(closest_low)]]\n",
    "        )\n",
    "        ht_to_highs.append(ht_to_high)\n",
    "        ht_to_lows.append(ht_to_low)\n",
    "\n",
    "        dist_to_low = event - closest_low\n",
    "        dist_to_highs.append(dist_to_high)\n",
    "        dist_to_lows.append(dist_to_low)\n",
    "        closest_highs.append(closest_high)\n",
    "        closest_lows.append(closest_low)\n",
    "\n",
    "        if abs(dist_to_high) < abs(dist_to_low):\n",
    "            high_closer = 1\n",
    "        else:\n",
    "            high_closer = 0\n",
    "        high_closers.append(high_closer)\n",
    "\n",
    "        if abs(ht_to_high) < abs(ht_to_low):\n",
    "            high_closer = 1\n",
    "        else:\n",
    "            high_closer = 0\n",
    "        high_ht_closers.append(high_closer)\n",
    "\n",
    "    dist_dict = {\n",
    "        \"dist_to_highs\": dist_to_highs,\n",
    "        \"dist_to_lows\": dist_to_lows,\n",
    "        \"closest_highs\": closest_highs,\n",
    "        \"closest_lows\": closest_lows,\n",
    "        \"high_closers\": high_closers,\n",
    "        \"ht_to_highs\": ht_to_highs,\n",
    "        \"ht_to_lows\": ht_to_lows,\n",
    "        \"high_ht_closers\": high_ht_closers,\n",
    "        \"vly_date\": vly_date,\n",
    "        \"vly_tide\": vly_tide,\n",
    "        \"pk_date\": pk_date,\n",
    "        \"pk_tide\": pk_tide,\n",
    "    }\n",
    "    return dist_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afcf7f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_gz05 = dists(ev_data, \"gz05\")\n",
    "dist_North = dists(ev_data, \"North\")\n",
    "dist_South = dists(ev_data, \"South\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687093b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dist_gz05[\"vly_date\"])\n",
    "print(dist_gz05[\"pk_date\"])\n",
    "spacing = 1  # Day\n",
    "dates = []\n",
    "initial_time = datetime.datetime.strptime(\"2010-12-12 00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "for i in range(365 * 11):\n",
    "    dates.append(initial_time + datetime.timedelta(days=spacing * i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28ffab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect axes using https://matplotlib.org/stable/users/explain/text/annotations.html\n",
    "from matplotlib.transforms import Bbox, TransformedBbox, blended_transform_factory\n",
    "\n",
    "from mpl_toolkits.axes_grid1.inset_locator import (\n",
    "    BboxPatch,\n",
    "    BboxConnector,\n",
    "    BboxConnectorPatch,\n",
    ")\n",
    "\n",
    "\n",
    "def connect_bbox(\n",
    "    bbox1, bbox2, loc1a, loc2a, loc1b, loc2b, prop_lines, prop_patches=None\n",
    "):\n",
    "    if prop_patches is None:\n",
    "        prop_patches = prop_lines.copy()\n",
    "        prop_patches[\"alpha\"] = prop_patches.get(\"alpha\", 1) * 0.2\n",
    "\n",
    "    c1 = BboxConnector(bbox1, bbox2, loc1=loc1a, loc2=loc2a, **prop_lines)\n",
    "    c1.set_clip_on(False)\n",
    "    c2 = BboxConnector(bbox1, bbox2, loc1=loc1b, loc2=loc2b, **prop_lines)\n",
    "    c2.set_clip_on(False)\n",
    "\n",
    "    bbox_patch1 = BboxPatch(bbox1, **prop_patches)\n",
    "    bbox_patch2 = BboxPatch(bbox2, **prop_patches)\n",
    "\n",
    "    p = BboxConnectorPatch(\n",
    "        bbox1,\n",
    "        bbox2,\n",
    "        # loc1a=3, loc2a=2, loc1b=4, loc2b=1,\n",
    "        loc1a=loc1a,\n",
    "        loc2a=loc2a,\n",
    "        loc1b=loc1b,\n",
    "        loc2b=loc2b,\n",
    "        **prop_patches,\n",
    "    )\n",
    "    p.set_clip_on(False)\n",
    "\n",
    "    return c1, c2, bbox_patch1, bbox_patch2, p\n",
    "\n",
    "\n",
    "def zoom_effect01(ax1, ax2, xmin, xmax, **kwargs):\n",
    "    \"\"\"\n",
    "    ax1 : the main axes\n",
    "    ax1 : the zoomed axes\n",
    "    (xmin,xmax) : the limits of the colored area in both plot axes.\n",
    "\n",
    "    connect ax1 & ax2. The x-range of (xmin, xmax) in both axes will\n",
    "    be marked.  The keywords parameters will be used ti create\n",
    "    patches.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    trans1 = blended_transform_factory(ax1.transData, ax1.transAxes)\n",
    "    trans2 = blended_transform_factory(ax2.transData, ax2.transAxes)\n",
    "\n",
    "    bbox = Bbox.from_extents(xmin, 0, xmax, 1)\n",
    "\n",
    "    mybbox1 = TransformedBbox(bbox, trans1)\n",
    "    mybbox2 = TransformedBbox(bbox, trans2)\n",
    "\n",
    "    prop_patches = kwargs.copy()\n",
    "    prop_patches[\"ec\"] = \"none\"\n",
    "    prop_patches[\"alpha\"] = 0.2\n",
    "\n",
    "    c1, c2, bbox_patch1, bbox_patch2, p = connect_bbox(\n",
    "        mybbox1,\n",
    "        mybbox2,\n",
    "        loc1a=3,\n",
    "        loc2a=2,\n",
    "        loc1b=4,\n",
    "        loc2b=1,\n",
    "        prop_lines=kwargs,\n",
    "        prop_patches=prop_patches,\n",
    "    )\n",
    "\n",
    "    # ax1.add_patch(bbox_patch1)\n",
    "    # ax2.add_patch(bbox_patch2)\n",
    "    ax2.add_patch(c1)\n",
    "    ax2.add_patch(c2)\n",
    "    ax2.add_patch(p)\n",
    "\n",
    "    return c1, c2, bbox_patch1, bbox_patch2, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "719f7979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get oslo colormap\n",
    "oslo = Colormap(\"crameri:oslo\").to_mpl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed53a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIGURE 3\n",
    "# Plot tides to see trends with diurnal tides\n",
    "def subplot(ax, place, dist, st, ed, triangles=False):\n",
    "    \"\"\"Subplot for Figure 3. Plots tides and events colored by time since last event\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax : matplotlib.pyplot.axis\n",
    "        Axis to plot on\n",
    "    place : str\n",
    "        Place tides are modeled at\n",
    "    dist : dict\n",
    "        Dict of high and low tides\n",
    "    st : datetime.datetime\n",
    "        Start time of panel\n",
    "    ed : datetime.datetime\n",
    "        End time of panel;\n",
    "    triangles : bool, optional\n",
    "        Flag to plot low and high tide triangles. Default is False.\n",
    "    \"\"\"\n",
    "    ax.plot(dates_timeseries, ev_data[f\"tides{place}\"], zorder=0, color=\"gainsboro\")\n",
    "    if triangles:\n",
    "        ax.scatter(\n",
    "            dist[\"pk_date\"], dist[\"pk_tide\"], zorder=0, color=\"gray\", marker=\"v\", s=100\n",
    "        )\n",
    "        ax.scatter(\n",
    "            dist[\"vly_date\"],\n",
    "            dist[\"vly_tide\"],\n",
    "            zorder=0,\n",
    "            color=\"gray\",\n",
    "            marker=\"^\",\n",
    "            s=100,\n",
    "        )\n",
    "    hr_since_last_ev = [\n",
    "        (a.days * 24 * 3600 + a.seconds) / 3600 for a in ev_data[\"time_since_last_ev\"]\n",
    "    ]\n",
    "    masked_hr = np.ma.masked_where(np.array(hr_since_last_ev) > 32, ev_data[\"ev_time\"])\n",
    "    cax = ax.scatter(\n",
    "        masked_hr,\n",
    "        ev_data[f\"tide_event_time_{place}\"],\n",
    "        c=hr_since_last_ev,\n",
    "        zorder=2,\n",
    "        cmap=\"viridis\",\n",
    "        vmin=0,\n",
    "        vmax=32,\n",
    "        s=100,\n",
    "    )\n",
    "    cbar = plt.colorbar(cax)\n",
    "    cbar.set_label(\"Time Since Last Event [hr]\", size=46)\n",
    "    cbar.ax.tick_params(labelsize=42)\n",
    "    ax.set_xlim(st, ed)\n",
    "    # ax.set_xlim(datetime.datetime(2012,12,8),datetime.datetime(2013,5,20))\n",
    "    ax.set_xlabel(\"Date [yyyy]\", size=55)\n",
    "    if triangles:\n",
    "        ax.set_xlabel(\"Date [yyyy-mm]\", size=55)\n",
    "    ax.set_ylabel(\"Tide Height [cm]\", size=55)\n",
    "    ax.xaxis.set_tick_params(labelsize=42)\n",
    "    ax.yaxis.set_tick_params(labelsize=42)\n",
    "\n",
    "    for start, interval in zip(no_data[\"starts\"][:], no_data[\"interval\"][:]):\n",
    "        start = datetime.datetime.strptime(start, \"%Y-%m-%d %H:%M:%S\")\n",
    "        rect = plt.Rectangle(\n",
    "            (start, -200), interval, 1000, facecolor=\"black\", alpha=0.06, zorder=3\n",
    "        )\n",
    "        ax1.add_patch(rect)\n",
    "\n",
    "    # ax.set_title(f'{place} Tide Reference',size=20)\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=[60, 10])\n",
    "st = datetime.datetime(2007, 12, 1)\n",
    "ed = datetime.datetime(2019, 6, 17)\n",
    "\n",
    "st = datetime.datetime(2008, 1, 1)\n",
    "ed = datetime.datetime(2019, 6, 1)\n",
    "\n",
    "subplot(ax1, \"gz05\", dist_gz05, st, ed, triangles=False)\n",
    "st = datetime.datetime(2013, 1, 1)\n",
    "ed = datetime.datetime(2013, 7, 1)\n",
    "rect = plt.Rectangle(\n",
    "    (st, -200),\n",
    "    ed - st,\n",
    "    1000,\n",
    "    facecolor=\"none\",\n",
    "    zorder=0,\n",
    "    alpha=0.4,\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=7,\n",
    ")\n",
    "ax1.add_patch(rect)\n",
    "\n",
    "\n",
    "subplot(ax2, \"gz05\", dist_gz05, st, ed, triangles=True)\n",
    "\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9228ba71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map\n",
    "fig, ax0 = plt.subplots()\n",
    "oslo = Colormap(\"crameri:oslo\").to_mpl()\n",
    "ax0.imshow(moa_dat, extent=ext, cmap=\"gray\", vmin=15000, vmax=17000)\n",
    "v = ax0.imshow(\n",
    "    vel_mag,\n",
    "    extent=(x_min_v, x_max_v, y_min_v, y_max_v),\n",
    "    cmap=oslo,\n",
    "    vmax=500,\n",
    "    vmin=0,\n",
    "    alpha=0.7,\n",
    ")\n",
    "for i, shape in enumerate(sf.shapes(bbox=bbox)):\n",
    "    if i > 0:\n",
    "        points = shape.points\n",
    "        # Only get points within bounding box\n",
    "        inbox = inBox(points, bbox)\n",
    "        line = LineString(inbox)\n",
    "        plot_line(line, ax=ax0, add_points=False, color=\"white\")\n",
    "x_min, x_max = bbox[0], bbox[2]\n",
    "y_min, y_max = bbox[1], bbox[3]\n",
    "ax0.set_xlim(x_min, x_max)\n",
    "ax0.set_ylim(y_min, y_max)\n",
    "KM_SCALE = 1e3\n",
    "ticks_x = ticker.FuncFormatter(lambda x, pos: \"{0:g}\".format(x / KM_SCALE))\n",
    "ax0.xaxis.set_major_formatter(ticks_x)\n",
    "ticks_y = ticker.FuncFormatter(lambda x, pos: \"{0:g}\".format(x / KM_SCALE))\n",
    "ax0.yaxis.set_major_formatter(ticks_y)\n",
    "ax0.set_xlabel(\"X (PS71) [km]\", size=20)\n",
    "ax0.set_ylabel(\"Y (PS71) [km]\", size=20)\n",
    "ax0.tick_params(labelsize=15)\n",
    "ax0.tick_params(size=4)\n",
    "# ax.set_facecolor(\"black\")\n",
    "cbarv = fig.colorbar(v)\n",
    "cbarv.ax.tick_params(labelsize=15)\n",
    "cbarv.ax.set_ylabel(\"Ice Velocity [m/a]\", size=20)\n",
    "\n",
    "lats = [-84.2986, y_north, y_south]\n",
    "lons = [-164.5206, x_north, x_south]\n",
    "places = [\"gz05\", \"North\", \"South\"]\n",
    "\n",
    "x, y = ll2xy(lons, lats)\n",
    "colors = [\"slategray\", \"gainsboro\", \"black\"]\n",
    "ax0.scatter(x, y, s=70, color=colors, marker=\"v\", edgecolors=\"white\")\n",
    "t = ax0.text(x[0], y[0] - 20000, \"gz05\", color=colors[0], size=20)\n",
    "t.set_bbox(dict(facecolor=\"white\", alpha=0.2, linewidth=0))\n",
    "t = ax0.text(x[1] - 40000, y[1] - 25000, \"North\", color=colors[1], size=20)\n",
    "t.set_bbox(dict(facecolor=\"white\", alpha=0.2, linewidth=0))\n",
    "t = ax0.text(x[2] - 60000, y[2] - 30000, \"South\", color=colors[2], size=20)\n",
    "t.set_bbox(dict(facecolor=\"white\", alpha=0.2, linewidth=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4481e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coordinates of grounding line\n",
    "from shapely.geometry import LineString\n",
    "from shapely.plotting import plot_line, plot_polygon\n",
    "\n",
    "# Results in arrays x_tide and y_tide, the x and y positions to calculated\n",
    "# grounding zone tides at. (Red dots in figure). Converted to lons and lats and\n",
    "# grouped as points in array lon_lats.\n",
    "# Creates polygons shelf, rumples, and island for use to determine if points are\n",
    "# on floating or grounded ice.\n",
    "\n",
    "# Importing grounding line and conversion to shapely adapted from\n",
    "# https://gis.stackexchange.com/questions/113799/reading-shapefile-in-python\n",
    "\n",
    "# Tuning parameters\n",
    "offset = 5000  # Distance from grounding line to offset, m\n",
    "pt_dist = 500  # Distance between tide sampling points, m\n",
    "\n",
    "# Arrays to pass to tides\n",
    "x_tide = []\n",
    "y_tide = []\n",
    "\n",
    "bbox = [-300000, -800000, -60000, -420000]  # Includes all of Crary\n",
    "fig, ax = plt.subplots(\n",
    "    figsize=[(bbox[2] - bbox[0]) / 100000 * 3, (bbox[3] - bbox[1]) / 100000 * 3]\n",
    ")\n",
    "gl_path = \"/mnt/c/Users/ZacharyKatz/Desktop/Research/Background/Antarctica_masksX/scripps_antarctica_polygons_v1.shp\"\n",
    "sf = shapefile.Reader(gl_path)\n",
    "for i, shape in enumerate(sf.shapes(bbox=bbox)):\n",
    "    if i > 0:\n",
    "        points = shape.points\n",
    "        # Only get points within bounding box\n",
    "        inbox = inBox(points, bbox)\n",
    "        line = LineString(inbox)\n",
    "        offset_line = line.offset_curve(offset)\n",
    "        plot_line(offset_line, ax=ax, add_points=False, color=\"gray\")\n",
    "\n",
    "        interpolated = interpolateLine(offset_line, pt_dist)\n",
    "        xs = [point.x for point in interpolated]\n",
    "        ys = [point.y for point in interpolated]\n",
    "        ax.scatter(xs, ys, s=1, color=\"red\")\n",
    "        x_tide.append(xs)\n",
    "        y_tide.append(ys)\n",
    "\n",
    "        if len(inbox) > 1000:  # Check if main array, not an island\n",
    "            x, y = line.xy\n",
    "            border = LineString(\n",
    "                [\n",
    "                    [x[0], y[0]],\n",
    "                    [-50000, -600000],\n",
    "                    [-50000, -800000],\n",
    "                    [-300000, -800000],\n",
    "                    [-300000, -700000],\n",
    "                    [x[-1], y[-1]],\n",
    "                ]\n",
    "            )\n",
    "            polygon = shapely.polygonize([line, border])\n",
    "            shelf = polygon.geoms[0]\n",
    "            plot_polygon(\n",
    "                shelf,\n",
    "                ax=ax,\n",
    "                add_points=False,\n",
    "                edgecolor=\"black\",\n",
    "                facecolor=\"white\",\n",
    "                alpha=0.7,\n",
    "            )\n",
    "        else:\n",
    "            polygon = shapely.polygonize([line])\n",
    "            if len(inbox) < 100:\n",
    "                rumples = polygon.geoms[0]\n",
    "                plot_polygon(rumples, ax=ax, add_points=False, color=\"black\")\n",
    "            else:\n",
    "                island = polygon.geoms[0]\n",
    "                plot_polygon(island, ax=ax, add_points=False, color=\"black\")\n",
    "\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(50000))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(50000))\n",
    "ax.set_xlabel(\"X [m]\", size=20)\n",
    "ax.set_ylabel(\"Y [m]\", size=20)\n",
    "ax.tick_params(labelsize=15)\n",
    "\n",
    "# Flatten the list. Not sure why this works but here's the source\n",
    "# https://www.scaler.com/topics/flatten-list-python/\n",
    "x_tide = sum(x_tide, [])\n",
    "y_tide = sum(y_tide, [])\n",
    "\n",
    "lon_lats = [xy2ll(x0, y0) for x0, y0 in zip(x_tide, y_tide)]\n",
    "lons = [x[0] for x in lon_lats]\n",
    "lats = [x[1] for x in lon_lats]\n",
    "print(lon_lats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e604131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup parameters\n",
    "import pyTMD\n",
    "import timescale\n",
    "\n",
    "(year1, month1, day1) = (2019, 3, 30)  # First day of gz_05 data\n",
    "# Calcluate date since Jan 1, 1992 [The format pyTMD wants it in]\n",
    "# minutes = np.arange(7 * 3 * 1440) # Calcuate tides every minute for 2 weeks\n",
    "hours = np.arange(1 * 24 * 2)  # Calcuate tides every minute for 2 weeks\n",
    "plot_t = np.arange(\n",
    "    datetime.datetime(2019, 3, 30),\n",
    "    datetime.datetime(2020, 3, 30),\n",
    "    datetime.timedelta(hours=0.5),\n",
    ").astype(datetime.datetime)\n",
    "tide_time = timescale.time.convert_calendar_dates(year1, month1, day1, hour=hours / 2)\n",
    "\n",
    "print(len(tide_time))\n",
    "print(len(plot_t))\n",
    "\n",
    "\n",
    "# Setup model\n",
    "dir = \"/mnt/c/Users/ZacharyKatz/Desktop/Research/Background\"\n",
    "mod = \"CATS2008-v2023\"\n",
    "model = pyTMD.io.model(dir, format=\"netcdf\").elevation(mod)\n",
    "print(model.format)\n",
    "constituents = pyTMD.io.OTIS.read_constants(\n",
    "    model.grid_file,\n",
    "    model.model_file,\n",
    "    model.projection,\n",
    "    type=model.type,\n",
    "    grid=model.format,\n",
    ")\n",
    "c = constituents.fields\n",
    "\n",
    "# CALCULATE TIDES ALONG BASIN #\n",
    "DELTAT = np.zeros_like(tide_time)\n",
    "amp, ph, D = pyTMD.io.OTIS.interpolate_constants(\n",
    "    np.atleast_1d(lons),\n",
    "    np.atleast_1d(lats),\n",
    "    constituents,\n",
    "    model.projection,\n",
    "    type=model.type,\n",
    "    method=\"spline\",\n",
    "    extrapolate=True,\n",
    ")\n",
    "# calculate complex phase in radians for Euler's\n",
    "cph = -1j * ph * np.pi / 180.0\n",
    "# calculate constituent oscillation\n",
    "hc = amp * np.exp(cph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "162de5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tides_new = []\n",
    "for i in range(len(tide_time)):\n",
    "    TIDE = pyTMD.predict.map(\n",
    "        tide_time[i], hc, c, deltat=DELTAT[i], corrections=model.format\n",
    "    )\n",
    "    MINOR = pyTMD.predict.infer_minor(\n",
    "        tide_time[i], hc, c, deltat=DELTAT[i], corrections=model.format\n",
    "    )\n",
    "    TIDE.data[:] += MINOR.data[:]\n",
    "    # convert to centimeters\n",
    "    TIDE.data[:] *= 100.0\n",
    "    tides_new.append(TIDE.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77f91cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subplotS3(ax, place, dist, st, ed, color, triangles=False):\n",
    "    \"\"\"Subplot for Figure 3. Plots tides and events colored by time since last event\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax : matplotlib.pyplot.axis\n",
    "        Axis to plot on\n",
    "    place : str\n",
    "        Place tides are modeled at\n",
    "    dist : dict\n",
    "        Dict of high and low tides\n",
    "    st : datetime.datetime\n",
    "        Start time of panel\n",
    "    ed : datetime.datetime\n",
    "        End time of panel;\n",
    "    triangles : bool, optional\n",
    "        Flag to plot low and high tide triangles. Default is False.\n",
    "    \"\"\"\n",
    "    if triangles:\n",
    "        ax.scatter(dist[\"pk_date\"], dist[\"pk_tide\"], zorder=0, color=\"gray\", marker=\"v\")\n",
    "        ax.scatter(\n",
    "            dist[\"vly_date\"], dist[\"vly_tide\"], zorder=0, color=\"gray\", marker=\"^\"\n",
    "        )\n",
    "    hr_since_last_ev = [\n",
    "        (a.days * 24 * 3600 + a.seconds) / 3600 for a in ev_data[\"time_since_last_ev\"]\n",
    "    ]\n",
    "    masked_hr = np.ma.masked_where(np.array(hr_since_last_ev) > 32, ev_data[\"ev_time\"])\n",
    "    cax = ax.scatter(\n",
    "        masked_hr,\n",
    "        ev_data[f\"tide_event_time_{place}\"],\n",
    "        c=hr_since_last_ev,\n",
    "        zorder=2,\n",
    "        cmap=\"viridis\",\n",
    "        vmin=0,\n",
    "        vmax=32,\n",
    "        s=100,\n",
    "    )\n",
    "    ax.set_xlim(st, ed)\n",
    "    # ax.set_xlim(datetime.datetime(2012,12,8),datetime.datetime(2013,5,20))\n",
    "    ax.set_xlabel(\"Date [yyyy]\", size=22)\n",
    "    if triangles:\n",
    "        ax.set_xlabel(\"Date [yyyy-mm-dd]\", size=30)\n",
    "\n",
    "    ax.set_ylabel(\"Tide Height [cm]\", size=22)\n",
    "    ax.xaxis.set_tick_params(labelsize=18)\n",
    "    ax.yaxis.set_tick_params(labelsize=18)\n",
    "\n",
    "    for start, interval in zip(no_data[\"starts\"][:], no_data[\"interval\"][:]):\n",
    "        start = datetime.datetime.strptime(start, \"%Y-%m-%d %H:%M:%S\")\n",
    "        rect = plt.Rectangle(\n",
    "            (start, -200), interval, 1000, facecolor=\"black\", alpha=0.1, zorder=3\n",
    "        )\n",
    "        ax1.add_patch(rect)\n",
    "    return cax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436d08cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static frame\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "bbox = [-300000, -800000, -60000, -420000]  # Includes all of Crary\n",
    "gl_path = \"/mnt/c/Users/ZacharyKatz/Desktop/Research/Background/Antarctica_masksX/scripps_antarctica_polygons_v1.shp\"\n",
    "sf = shapefile.Reader(gl_path)\n",
    "fig, ax = plt.subplots(\n",
    "    figsize=[(bbox[2] - bbox[0]) / 100000 * 5, (bbox[3] - bbox[1]) / 100000 * 3],\n",
    "    constrained_layout=True,\n",
    ")\n",
    "ax.imshow(moa_dat, extent=ext, cmap=\"gray\", vmin=15000, vmax=17000)\n",
    "v = ax.imshow(\n",
    "    vel_mag,\n",
    "    extent=(x_min_v, x_max_v, y_min_v, y_max_v),\n",
    "    cmap=oslo,\n",
    "    vmax=500,\n",
    "    vmin=0,\n",
    "    alpha=0.7,\n",
    ")\n",
    "tide = ax.scatter(x_tide, y_tide, c=tides_new[3], vmax=100, vmin=-100, cmap=\"PiYG_r\")\n",
    "for i, shape in enumerate(sf.shapes(bbox=bbox)):\n",
    "    if i > 0:\n",
    "        points = shape.points\n",
    "        # Only get points within bounding box\n",
    "        inbox = inBox(points, bbox)\n",
    "        line = LineString(inbox)\n",
    "        plot_line(line, ax=ax, add_points=False, color=\"black\")\n",
    "x_min, x_max = bbox[0], bbox[2]\n",
    "y_min, y_max = bbox[1], bbox[3]\n",
    "ax.set_xlim(x_min, x_max)\n",
    "ax.set_ylim(y_min, y_max)\n",
    "KM_SCALE = 1e3\n",
    "ticks_x = ticker.FuncFormatter(lambda x, pos: \"{0:g}\".format(x / KM_SCALE))\n",
    "ax.xaxis.set_major_formatter(ticks_x)\n",
    "ticks_y = ticker.FuncFormatter(lambda x, pos: \"{0:g}\".format(x / KM_SCALE))\n",
    "ax.yaxis.set_major_formatter(ticks_y)\n",
    "ax.set_xlabel(\"X (PS71) [km]\", size=30)\n",
    "ax.set_ylabel(\"Y (PS71) [km]\", size=30)\n",
    "ax.tick_params(labelsize=25)\n",
    "ax.tick_params(size=4)\n",
    "# ax.set_facecolor(\"black\")\n",
    "# Place colorbar in top right of figur\n",
    "# Adapted from\n",
    "# https://stackoverflow.com/questions/35950559/add-a-white-background-to-colorbar-in-matplotlib\n",
    "\n",
    "\n",
    "cbaxes = inset_axes(ax, width=\"40%\", height=\"10%\", loc=\"upper right\")\n",
    "[cbaxes.spines[k].set_visible(False) for k in cbaxes.spines]\n",
    "cbaxes.tick_params(\n",
    "    axis=\"both\",\n",
    "    left=False,\n",
    "    top=False,\n",
    "    right=False,\n",
    "    bottom=False,\n",
    "    labelleft=False,\n",
    "    labeltop=False,\n",
    "    labelright=False,\n",
    "    labelbottom=False,\n",
    ")\n",
    "cbaxes.set_facecolor([0, 0, 0, 0.5])\n",
    "cbbox = inset_axes(cbaxes, width=\"80%\", height=\"16%\", loc=\"center\")\n",
    "cbarv = fig.colorbar(v, cax=cbbox, orientation=\"horizontal\")\n",
    "cbarv.outline.set_edgecolor(\"white\")\n",
    "cbarv.ax.tick_params(labelsize=18, color=\"white\", labelcolor=\"white\")\n",
    "cbarv.ax.set_xlabel(\"Ice Velocity [m a$^{-1}$]\", size=18, color=\"white\", labelpad=-58)\n",
    "\n",
    "cbaramp = fig.colorbar(tide)\n",
    "cbaramp.ax.tick_params(labelsize=25)\n",
    "cbaramp.ax.set_ylabel(\"Tidal Amplitude [cm]\", size=30)\n",
    "\n",
    "lats = [-84.2986, y_north, y_south]\n",
    "lons = [-164.5206, x_north, x_south]\n",
    "places = [\"gz05\", \"North\", \"South\"]\n",
    "\n",
    "x, y = ll2xy(lons, lats)\n",
    "colors = [\"#edb3a5\", \"#a03820\", \"black\"]\n",
    "ax.scatter(x, y, s=170, color=colors, marker=\"v\", edgecolors=\"black\")\n",
    "t = ax.text(x[0] + 5000, y[0] - 15000, \"GZ05\", color=colors[0], size=30)\n",
    "t.set_bbox(dict(facecolor=\"white\", alpha=0.2, linewidth=0))\n",
    "t = ax.text(x[1] - 20000, y[1] - 20000, \"North\", color=colors[1], size=30)\n",
    "t.set_bbox(dict(facecolor=\"white\", alpha=0.2, linewidth=0))\n",
    "t = ax.text(x[2] - 45000, y[2] - 15000, \"South\", color=colors[2], size=30)\n",
    "t.set_bbox(dict(facecolor=\"white\", alpha=0.2, linewidth=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2023889",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y)\n",
    "cbaxes = inset_axes(ax, width=\"30%\", height=\"3%\", loc=\"upper right\")\n",
    "cbaxes.set_facecolor(\"green\")\n",
    "cbarv = fig.colorbar(v, cax=cbaxes, orientation=\"horizontal\")\n",
    "# Set inset axes background color\\\\\n",
    "\n",
    "a = np.random.rand(10, 10)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "im = ax.imshow(a)\n",
    "\n",
    "cbbox = inset_axes(ax, \"15%\", \"90%\", loc=7)\n",
    "[cbbox.spines[k].set_visible(False) for k in cbbox.spines]\n",
    "cbbox.tick_params(\n",
    "    axis=\"both\",\n",
    "    left=\"off\",\n",
    "    top=\"off\",\n",
    "    right=\"off\",\n",
    "    bottom=\"off\",\n",
    "    labelleft=\"off\",\n",
    "    labeltop=\"off\",\n",
    "    labelright=\"off\",\n",
    "    labelbottom=\"off\",\n",
    ")\n",
    "cbbox.set_facecolor([1, 1, 1, 0.7])\n",
    "\n",
    "cbaxes = inset_axes(cbbox, \"30%\", \"95%\", loc=6)\n",
    "\n",
    "cb = fig.colorbar(im, cax=cbaxes)  # make colorbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b54bba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just the timeseries\n",
    "fig, ax2 = plt.subplots(figsize=[18, 8])\n",
    "\n",
    "st = datetime.datetime(2013, 3, 12, 6)\n",
    "ed = datetime.datetime(2013, 3, 21, 18)\n",
    "\n",
    "ax2.plot(\n",
    "    dates_timeseries,\n",
    "    ev_data[\"tidesSouth\"],\n",
    "    zorder=0,\n",
    "    color=colors[2],\n",
    "    label=\"South\",\n",
    "    linewidth=2,\n",
    ")\n",
    "ax2.plot(\n",
    "    dates_timeseries,\n",
    "    ev_data[\"tidesgz05\"],\n",
    "    zorder=0,\n",
    "    color=colors[0],\n",
    "    label=\"GZ05\",\n",
    "    linewidth=2,\n",
    ")\n",
    "ax2.plot(\n",
    "    dates_timeseries,\n",
    "    ev_data[\"tidesNorth\"],\n",
    "    zorder=0,\n",
    "    color=colors[1],\n",
    "    label=\"North\",\n",
    "    linewidth=2,\n",
    ")\n",
    "\n",
    "cax = subplotS3(ax2, \"gz05\", dist_gz05, st, ed, colors[0], triangles=True)\n",
    "cax = subplotS3(ax2, \"North\", dist_North, st, ed, colors[1], triangles=True)\n",
    "cax = subplotS3(ax2, \"South\", dist_South, st, ed, colors[2], triangles=True)\n",
    "cbar = plt.colorbar(cax)  # location='bottom',shrink=0.4)\n",
    "cbar.set_label(\"Time Since Last Event [hr]\", size=28)\n",
    "cbar.ax.tick_params(labelsize=25)\n",
    "\n",
    "ax2.set_ylabel(\"Tide Height [cm]\", size=30)\n",
    "for i, label in enumerate(ax2.get_xticklabels()):\n",
    "    label.set_visible(i % 2 == 0)\n",
    "ax2.legend(fontsize=25)\n",
    "\n",
    "ax2.set_ylim(-125, 125)\n",
    "\n",
    "ax2.tick_params(labelsize=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e95850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIGURE S3\n",
    "# Overall size based on map aspect\n",
    "fig = plt.figure(figsize=[16, 16])\n",
    "gs = mpl.gridspec.GridSpec(6, 6, wspace=2, hspace=0.6)\n",
    "ax0 = fig.add_subplot(gs[0:3, 0:3])\n",
    "ax1 = fig.add_subplot(gs[0:3, 3:5], aspect=\"equal\")\n",
    "ax2 = fig.add_subplot(gs[3:5, 0:6])\n",
    "cbar_ax = fig.add_subplot(gs[0:3, 2])\n",
    "cbar_ax.axis(\"off\")\n",
    "\n",
    "# Map\n",
    "oslo = Colormap(\"crameri:oslo\").to_mpl()\n",
    "ax0.imshow(moa_dat, extent=ext, cmap=\"gray\", vmin=15000, vmax=17000)\n",
    "v = ax0.imshow(\n",
    "    vel_mag,\n",
    "    extent=(x_min_v, x_max_v, y_min_v, y_max_v),\n",
    "    cmap=oslo,\n",
    "    vmax=500,\n",
    "    vmin=0,\n",
    "    alpha=0.7,\n",
    ")\n",
    "for i, shape in enumerate(sf.shapes(bbox=bbox)):\n",
    "    if i > 0:\n",
    "        points = shape.points\n",
    "        # Only get points within bounding box\n",
    "        inbox = inBox(points, bbox)\n",
    "        line = LineString(inbox)\n",
    "        plot_line(line, ax=ax0, add_points=False, color=\"white\")\n",
    "x_min, x_max = bbox[0], bbox[2]\n",
    "y_min, y_max = bbox[1], bbox[3]\n",
    "ax0.set_xlim(x_min, x_max)\n",
    "ax0.set_ylim(y_min, y_max)\n",
    "KM_SCALE = 1e3\n",
    "ticks_x = ticker.FuncFormatter(lambda x, pos: \"{0:g}\".format(x / KM_SCALE))\n",
    "ax0.xaxis.set_major_formatter(ticks_x)\n",
    "ticks_y = ticker.FuncFormatter(lambda x, pos: \"{0:g}\".format(x / KM_SCALE))\n",
    "ax0.yaxis.set_major_formatter(ticks_y)\n",
    "ax0.set_xlabel(\"X (PS71) [km]\", size=20)\n",
    "ax0.set_ylabel(\"Y (PS71) [km]\", size=20)\n",
    "ax0.tick_params(labelsize=15)\n",
    "ax0.tick_params(size=4)\n",
    "# ax.set_facecolor(\"black\")\n",
    "cbarv = fig.colorbar(v)\n",
    "cbarv.ax.tick_params(labelsize=15)\n",
    "cbarv.ax.set_ylabel(\"Ice Velocity [m/a]\", size=20)\n",
    "\n",
    "lats = [-84.2986, y_north, y_south]\n",
    "lons = [-164.5206, x_north, x_south]\n",
    "places = [\"gz05\", \"North\", \"South\"]\n",
    "\n",
    "x, y = ll2xy(lons, lats)\n",
    "colors = [\"#edb3a5\", \"#D34B29\", \"black\"]\n",
    "ax0.scatter(x, y, s=70, color=colors, marker=\"v\", edgecolors=\"white\")\n",
    "t = ax0.text(x[0], y[0] - 25000, \"GZ05\", color=colors[0], size=20)\n",
    "t.set_bbox(dict(facecolor=\"white\", alpha=0.2, linewidth=0))\n",
    "t = ax0.text(x[1] - 25000, y[1] - 25000, \"North\", color=colors[1], size=20)\n",
    "t.set_bbox(dict(facecolor=\"white\", alpha=0.2, linewidth=0))\n",
    "t = ax0.text(x[2] - 60000, y[2] - 25000, \"South\", color=colors[2], size=20)\n",
    "t.set_bbox(dict(facecolor=\"white\", alpha=0.2, linewidth=0))\n",
    "\n",
    "\n",
    "# Scatterplot\n",
    "hr_since_last_ev = [\n",
    "    (a.days * 24 * 3600 + a.seconds) / 3600 for a in ev_data[\"time_since_last_ev\"]\n",
    "]\n",
    "masked_hr = np.ma.masked_where(np.array(hr_since_last_ev) > 32, ev_data[\"ev_time\"])\n",
    "cax = ax1.scatter(\n",
    "    ev_data[\"tide_event_time_South\"],\n",
    "    ev_data[\"tide_event_time_North\"],\n",
    "    c=hr_since_last_ev,\n",
    "    zorder=2,\n",
    "    cmap=\"viridis\",\n",
    "    vmin=0,\n",
    "    vmax=32,\n",
    ")\n",
    "\n",
    "ax1.set_xlabel(\"Tide at South [cm]\", size=20)\n",
    "ax1.set_ylabel(\"Tide at North [cm]\", size=20)\n",
    "ax1.set_xlim(-150, 150)\n",
    "ax1.set_ylim(-150, 150)\n",
    "x = np.arange(-150, 250, 150)\n",
    "y = np.arange(-150, 250, 150)\n",
    "ax1.plot(y, x, \"red\")\n",
    "ax1.tick_params(labelsize=15)\n",
    "ax1.tick_params(size=4)\n",
    "\n",
    "st = datetime.datetime(2013, 3, 12)\n",
    "ed = datetime.datetime(2013, 3, 22)\n",
    "\n",
    "ax2.plot(\n",
    "    dates_timeseries, ev_data[\"tidesSouth\"], zorder=0, color=colors[2], label=\"South\"\n",
    ")\n",
    "ax2.plot(\n",
    "    dates_timeseries, ev_data[\"tidesgz05\"], zorder=0, color=colors[0], label=\"GZ05\"\n",
    ")\n",
    "ax2.plot(\n",
    "    dates_timeseries, ev_data[\"tidesNorth\"], zorder=0, color=colors[1], label=\"North\"\n",
    ")\n",
    "\n",
    "cax = subplotS3(ax2, \"gz05\", dist_gz05, st, ed, colors[0], triangles=True)\n",
    "cax = subplotS3(ax2, \"North\", dist_North, st, ed, colors[1], triangles=True)\n",
    "cax = subplotS3(ax2, \"South\", dist_South, st, ed, colors[2], triangles=True)\n",
    "cbar = plt.colorbar(cax)\n",
    "cbar.set_label(\"Time Since Last Event [hr]\", size=22)\n",
    "cbar.ax.tick_params(labelsize=18)\n",
    "\n",
    "ax2.set_ylabel(\"Tide Height [cm]\", size=20)\n",
    "for i, label in enumerate(ax2.get_xticklabels()):\n",
    "    label.set_visible(i % 2 == 0)\n",
    "ax2.legend()\n",
    "ax2.set_ylim(-125, 125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df68495a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find time since last event for all skipped low tides\n",
    "time_since_last_event_low_pk = []\n",
    "time_to_next_event_low_pk = []\n",
    "\n",
    "events = np.ma.masked_where(\n",
    "    np.array(dist_gz05[\"high_closers\"]) == 1, ev_data[\"ev_time\"]\n",
    ")\n",
    "events = np.ma.compressed(events)\n",
    "tide_low = np.ma.masked_where(\n",
    "    np.array(dist_gz05[\"high_closers\"]) == 1, ev_data[\"tide_event_time_gz05\"]\n",
    ")\n",
    "tide_low = np.ma.compressed(tide_low)\n",
    "\n",
    "for i, valley in enumerate(dist_gz05[\"vly_date\"]):\n",
    "    closest_event = min(\n",
    "        events,\n",
    "        key=lambda d: abs(datetime.datetime.strptime(d, \"%Y-%m-%d %H:%M:%S\") - valley),\n",
    "    )\n",
    "    if datetime.datetime.strptime(closest_event, \"%Y-%m-%d %H:%M:%S\") > valley:\n",
    "        closest_event = events[np.where(events == closest_event)[0] - 1][0]\n",
    "    time_since_last_event_low_pk.append(\n",
    "        valley - datetime.datetime.strptime(closest_event, \"%Y-%m-%d %H:%M:%S\")\n",
    "    )\n",
    "\n",
    "    closest_event = min(\n",
    "        events,\n",
    "        key=lambda d: abs(datetime.datetime.strptime(d, \"%Y-%m-%d %H:%M:%S\") - valley),\n",
    "    )\n",
    "    if datetime.datetime.strptime(closest_event, \"%Y-%m-%d %H:%M:%S\") < valley:\n",
    "        try:\n",
    "            closest_event = events[np.where(events == closest_event)[0] + 1][0]\n",
    "        except IndexError:\n",
    "            pass\n",
    "    time_to_next_event_low_pk.append(\n",
    "        datetime.datetime.strptime(closest_event, \"%Y-%m-%d %H:%M:%S\") - valley\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da49cad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_average(\n",
    "    window,\n",
    "    slide_len,\n",
    "    time_since_last_event_low_pk,\n",
    "    time_to_next_event_low_pk,\n",
    "    dist_gz05,\n",
    "    ev_data,\n",
    "    no_data,\n",
    "):\n",
    "    \"\"\"Rolling average of slip events.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    window : int\n",
    "        Days to average over\n",
    "    slide_len : int\n",
    "        Days to slide window\n",
    "    time_since_last_event_low_pk : list\n",
    "        Times since last event\n",
    "    time_to_next_event_low_pk : list\n",
    "        Time to next event\n",
    "    dist_gz05 : dict\n",
    "        Dict of high and low tides\n",
    "    ev_data : dict\n",
    "        Dict of event data\n",
    "    no_data : dict\n",
    "        Dict of no data times\n",
    "    \"\"\"\n",
    "    # Compute rolling average of skipped low slips\n",
    "\n",
    "    time_since_low_hr = [a.total_seconds() / 3600 for a in time_since_last_event_low_pk]\n",
    "    time_to_low_hr = [a.total_seconds() / 3600 for a in time_to_next_event_low_pk]\n",
    "\n",
    "    mask = []\n",
    "    for ts, tt in zip(time_since_low_hr, time_to_low_hr):\n",
    "        if ts < 6 or tt < 6:\n",
    "            mask.append(1)\n",
    "        else:\n",
    "            mask.append(0)\n",
    "\n",
    "    masked_low_skips = np.ma.masked_where(mask, dist_gz05[\"vly_date\"])\n",
    "    masked_low_skips = np.ma.compressed(masked_low_skips)\n",
    "\n",
    "    dates_timeseries_avg = []\n",
    "    initial_time = datetime.datetime.strptime(\n",
    "        \"2007-12-01 00:00:00\", \"%Y-%m-%d %H:%M:%S\"\n",
    "    )\n",
    "\n",
    "    time_to_append = window  # Days\n",
    "    slide = slide_len  # Days\n",
    "\n",
    "    # assert time_to_append % slide == 0\n",
    "    interval = int(time_to_append // slide)  # MUST BE AN INTEGER\n",
    "\n",
    "    rng = int(12 * 365 // slide)  # Range of times\n",
    "    for i in range(rng):\n",
    "        dates_timeseries_avg.append(initial_time + datetime.timedelta(days=slide * i))\n",
    "\n",
    "    skips = []\n",
    "    skip_starts = []\n",
    "    skip_ends = []\n",
    "    skip_middles = []\n",
    "    tot_slips = []\n",
    "    spacings = []\n",
    "\n",
    "    for i, date in enumerate(dates_timeseries_avg):\n",
    "        if i > interval:\n",
    "            interval_start = dates_timeseries_avg[i - interval]\n",
    "            interval_end = date\n",
    "\n",
    "            # print(interval_start,interval_end)\n",
    "            last_ev = datetime.datetime.strptime(\n",
    "                ev_data[\"ev_time\"].iloc[-1], \"%Y-%m-%d %H:%M:%S\"\n",
    "            )\n",
    "            first_ev = datetime.datetime.strptime(\n",
    "                ev_data[\"ev_time\"].iloc[0], \"%Y-%m-%d %H:%M:%S\"\n",
    "            )\n",
    "            stop = False\n",
    "            for start, end in zip(no_data[\"starts\"], no_data[\"ends\"]):\n",
    "                start = datetime.datetime.strptime(start, \"%Y-%m-%d %H:%M:%S\")\n",
    "                end = datetime.datetime.strptime(end, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "                # Check that interval does not include gaps in data or start before 1st event or end after last event\n",
    "                # if (interval_start > start and interval_start < end) or (interval_end > start and interval_end < end) \\\n",
    "                #    or (interval_start < end and interval_end > start) or (interval_start < start and interval_end > start) \\\n",
    "                #        or interval_start < first_ev \\\n",
    "                #        or interval_end > last_ev:\n",
    "                #    stop = True\n",
    "\n",
    "                if (\n",
    "                    (interval_start < end and interval_end > start)\n",
    "                    or interval_start < first_ev\n",
    "                    or interval_end > last_ev\n",
    "                ):\n",
    "                    stop = True\n",
    "\n",
    "            if not stop:\n",
    "                # Number of total events\n",
    "                slip = 0\n",
    "                for event in ev_data[\"ev_time\"]:\n",
    "                    event = datetime.datetime.strptime(event, \"%Y-%m-%d %H:%M:%S\")\n",
    "                    if event > interval_start and event < interval_end:\n",
    "                        slip += 1\n",
    "\n",
    "                # Number of low skips\n",
    "                skip = 0\n",
    "                for date_low in masked_low_skips:\n",
    "                    if date_low > interval_start and date_low < interval_end:\n",
    "                        skip += 1\n",
    "\n",
    "                # Avg Event spacing\n",
    "                spacing = 0\n",
    "                counter = 0\n",
    "                time_since = [\n",
    "                    (a.days * 24 * 3600 + a.seconds) / 3600\n",
    "                    for a in ev_data[\"time_since_last_ev\"]\n",
    "                ]\n",
    "                for f, event in enumerate(ev_data[\"ev_time\"]):\n",
    "                    event = datetime.datetime.strptime(event, \"%Y-%m-%d %H:%M:%S\")\n",
    "                    if event > interval_start and event < interval_end:\n",
    "                        spacing += time_since[f]\n",
    "                        counter += 1\n",
    "                try:\n",
    "                    spacing = spacing / counter\n",
    "                except ZeroDivisionError:\n",
    "                    spacing = np.nan\n",
    "                # Avg Event amplitude\n",
    "\n",
    "            else:\n",
    "                skip = np.nan\n",
    "                slip = np.nan\n",
    "                spacing = np.nan\n",
    "\n",
    "            skip_starts.append(interval_start)\n",
    "            skip_ends.append(interval_end)\n",
    "            skip_middles.append(interval_start + (interval_end - interval_start) / 2)\n",
    "            skips.append(skip)\n",
    "            tot_slips.append(slip)\n",
    "            spacings.append(spacing)\n",
    "\n",
    "    return skip_starts, skip_ends, skip_middles, skips, tot_slips, spacings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe84dff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotation(ax, x1, x2, y1, y2, text):\n",
    "    \"\"\"Draws an annotated bracket.\"\"\"\n",
    "    xs = [x1, x1, x2, x2]\n",
    "    ys = [y1, y2, y2, y1]\n",
    "    ax.plot(xs, ys, \"k-\")\n",
    "    ax.text((x2 - x1) / 2 + x1, y2 + 0.1, text, ha=\"center\", va=\"bottom\", fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83d32527",
   "metadata": {},
   "outputs": [],
   "source": [
    "slide = 1\n",
    "rolling_avg_dicts = []\n",
    "for window in [3, 27, 45]:\n",
    "    skip_starts, skip_ends, skip_middles, skips, tot_slips, spacings = rolling_average(\n",
    "        window,\n",
    "        slide,\n",
    "        time_since_last_event_low_pk,\n",
    "        time_to_next_event_low_pk,\n",
    "        dist_gz05,\n",
    "        ev_data,\n",
    "        no_data,\n",
    "    )\n",
    "    # Save as dict\n",
    "    rolling_avg_dict = {\n",
    "        \"skip_starts\": skip_starts,\n",
    "        \"skip_ends\": skip_ends,\n",
    "        \"skip_middles\": skip_middles,\n",
    "        \"skips\": skips,\n",
    "        \"tot_slips\": tot_slips,\n",
    "        \"spacings\": spacings,\n",
    "    }\n",
    "    rolling_avg_dicts.append(rolling_avg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c30aa3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting function\n",
    "def plot_rolling_avg_section(\n",
    "    ax1,\n",
    "    ax3,\n",
    "    window,\n",
    "    slide_len,\n",
    "    skip_starts,\n",
    "    skip_ends,\n",
    "    skip_middles,\n",
    "    skips,\n",
    "    tot_slips,\n",
    "    spacings,\n",
    "):\n",
    "    # Skipped slips by number of slips in xx period\n",
    "    ax2 = ax1.twinx()\n",
    "    (\n",
    "        ax1.plot(\n",
    "            dates_timeseries, ev_data[\"tidesgz05\"], zorder=1, color=\"gray\", alpha=0.4\n",
    "        ),\n",
    "    )\n",
    "    for start, interval in zip(no_data[\"starts\"][:], no_data[\"interval\"][:]):\n",
    "        start = datetime.datetime.strptime(start, \"%Y-%m-%d %H:%M:%S\")\n",
    "        rect = plt.Rectangle(\n",
    "            (start, -200), interval, 1000, facecolor=\"black\", alpha=0.2, zorder=3\n",
    "        )\n",
    "        ax1.add_patch(rect)\n",
    "\n",
    "    ax1.set_xlim(datetime.datetime(2008, 1, 1), datetime.datetime(2019, 6, 1))\n",
    "    ax1.set_ylim(-120, 130)\n",
    "\n",
    "    xerr = [(end - start) // 2 for start, end in zip(skip_starts, skip_ends)]\n",
    "    norm_slips = [i / window for i in tot_slips]\n",
    "    ax2.errorbar(skip_middles, norm_slips, xerr=xerr, fmt=\"o\", color=\"black\", zorder=2)\n",
    "    # ax2.plot(skip_middles, tot_slips, color='blue')\n",
    "\n",
    "    timestamps = [a.timestamp() for a in skip_middles]\n",
    "\n",
    "    compressed_tot_slips = np.ma.masked_invalid(tot_slips)\n",
    "    compressed_timestamps = np.ma.masked_where(\n",
    "        np.ma.getmask(compressed_tot_slips), timestamps\n",
    "    )\n",
    "    compressed_tot_slips = np.ma.compressed(compressed_tot_slips)\n",
    "    compressed_timestamps = np.ma.compressed(compressed_timestamps)\n",
    "\n",
    "    coef = np.polyfit(compressed_timestamps, compressed_tot_slips, 1)\n",
    "    print(coef)\n",
    "    lst_sqrs_fit = np.poly1d(coef)\n",
    "    print(lst_sqrs_fit[1])\n",
    "\n",
    "    datetimes = [datetime.datetime.fromtimestamp(a) for a in compressed_timestamps]\n",
    "    # ax2.plot(datetimes, lst_sqrs_fit(compressed_timestamps),color='blue',zorder=3)\n",
    "\n",
    "    # Just a line so manually extract slope\n",
    "    delta_y = (\n",
    "        lst_sqrs_fit(compressed_timestamps)[-1] - lst_sqrs_fit(compressed_timestamps)[0]\n",
    "    )\n",
    "    delta_x = datetimes[-1] - datetimes[0]\n",
    "\n",
    "    print(delta_y, delta_x)\n",
    "    slope = delta_y / delta_x.total_seconds() * 3600 * 24 * 365\n",
    "    print(slope)  # Slips / year\n",
    "    ax1.yaxis.set_label_position(\"right\")\n",
    "    ax1.yaxis.tick_right()\n",
    "    ax2.yaxis.set_label_position(\"left\")\n",
    "    ax2.yaxis.tick_left()\n",
    "    ax1.set_xlabel(\"Date [yyyy]\", size=22)\n",
    "    ax1.set_ylabel(\"Tide Height [cm]\", size=22)\n",
    "    ax2.set_ylabel(f\"Slips/Day [{window} Day Window]\", size=22, color=\"black\")\n",
    "    ax1.xaxis.set_tick_params(labelsize=18)\n",
    "    ax1.yaxis.set_tick_params(labelsize=18, color=\"gray\")\n",
    "    ax2.yaxis.set_tick_params(labelsize=18, colors=\"black\")\n",
    "    # ax2.spines['left'].set_color('red')\n",
    "\n",
    "    ax4 = ax3.twinx()\n",
    "    (\n",
    "        ax3.plot(\n",
    "            dates_timeseries, ev_data[\"tidesgz05\"], zorder=1, color=\"gray\", alpha=0.4\n",
    "        ),\n",
    "    )\n",
    "    for start, interval in zip(no_data[\"starts\"][:], no_data[\"interval\"][:]):\n",
    "        start = datetime.datetime.strptime(start, \"%Y-%m-%d %H:%M:%S\")\n",
    "        rect = plt.Rectangle(\n",
    "            (start, -200), interval, 1000, facecolor=\"black\", alpha=0.2, zorder=3\n",
    "        )\n",
    "        ax3.add_patch(rect)\n",
    "\n",
    "    st3 = datetime.datetime(2014, 1, 1)\n",
    "    ed3 = datetime.datetime(2015, 6, 1)\n",
    "\n",
    "    ax3.set_xlim(st3, ed3)\n",
    "    # ax3.set_xlim(datetime.datetime(2008,12,8),datetime.datetime(2009,7,1))\n",
    "    ax3.set_ylim(-155, 130)\n",
    "\n",
    "    xerr = [(end - start) // 2 for start, end in zip(skip_starts, skip_ends)]\n",
    "    # Segmet skipmiddles and norm_slips by date\n",
    "    mask = [start > st3 and end < ed3 for start, end in zip(skip_middles, skip_ends)]\n",
    "    skip_middles = np.array(skip_middles)[mask]\n",
    "    norm_slips = np.array(norm_slips)[mask]\n",
    "    xerr = np.array(xerr)[mask]\n",
    "\n",
    "    ax4.errorbar(skip_middles, norm_slips, xerr=xerr, fmt=\"o\", color=\"black\", zorder=2)\n",
    "    ax3.yaxis.set_label_position(\"right\")\n",
    "    ax3.yaxis.tick_right()\n",
    "    ax4.yaxis.set_label_position(\"left\")\n",
    "    ax4.yaxis.tick_left()\n",
    "    ax3.set_xlabel(\"Date [yyyy-mm]\", size=22)\n",
    "    ax3.set_ylabel(\"Tide Height [cm]\", size=22)\n",
    "    ax4.set_ylabel(f\"Slips/Day [{window} Day Window]\", size=22, color=\"black\")\n",
    "    ax3.xaxis.set_tick_params(labelsize=18)\n",
    "    ax3.yaxis.set_tick_params(labelsize=18, color=\"gray\")\n",
    "    ax4.yaxis.set_tick_params(labelsize=18, colors=\"black\")\n",
    "\n",
    "    ssa_time = datetime.timedelta(days=182.63)\n",
    "    ssa_start = datetime.datetime(2014, 3, 1)\n",
    "    annotation(ax1, ssa_start, ssa_start + ssa_time, 92, 96, \"$S_{sa}$\")\n",
    "\n",
    "    mm_time = datetime.timedelta(days=27.55)\n",
    "    mm_start = datetime.datetime(2014, 6, 11)\n",
    "    annotation(ax3, mm_start, mm_start + mm_time, 94, 98, \"$M_{m}$\")\n",
    "\n",
    "    # ax1.set_title(f'{time_to_append} Day Rolling Average',size=22)\n",
    "\n",
    "    ax1.set_title(f\"{window} Day Window\", size=22)\n",
    "    # fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0b564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot FIGS 4, S4, S5\n",
    "\n",
    "SEC_TO_DAY = 3600 * 24\n",
    "SEC_TO_HR = 3600\n",
    "xlim = 450\n",
    "\n",
    "viridis = mpl.colormaps[\"viridis\"].resampled(20)\n",
    "# Add window to rolling avg dict\n",
    "for i, window in enumerate([3, 27, 45]):\n",
    "    rolling_avg_dicts[i][\"window\"] = window\n",
    "\n",
    "\n",
    "for rolling_avg_dict in rolling_avg_dicts:\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(\n",
    "        3,\n",
    "        1,\n",
    "        figsize=[20, 20],\n",
    "        gridspec_kw={\"height_ratios\": [1, 1, 1.5]},\n",
    "        layout=\"constrained\",\n",
    "    )\n",
    "    skip_starts = rolling_avg_dict[\"skip_starts\"]\n",
    "    skip_ends = rolling_avg_dict[\"skip_ends\"]\n",
    "    skip_middles = rolling_avg_dict[\"skip_middles\"]\n",
    "    skips = rolling_avg_dict[\"skips\"]\n",
    "    tot_slips = rolling_avg_dict[\"tot_slips\"]\n",
    "    spacings = rolling_avg_dict[\"spacings\"]\n",
    "    window = rolling_avg_dict[\"window\"]\n",
    "    plot_rolling_avg_section(\n",
    "        ax1,\n",
    "        ax2,\n",
    "        window,\n",
    "        slide,\n",
    "        skip_starts,\n",
    "        skip_ends,\n",
    "        skip_middles,\n",
    "        skips,\n",
    "        tot_slips,\n",
    "        spacings,\n",
    "    )\n",
    "\n",
    "    # Lomb Scargle\n",
    "    dates_as_sec = [a.timestamp() for a in skip_middles]\n",
    "    tot_slips_masked = np.ma.masked_array(\n",
    "        tot_slips, mask=np.invert(~np.isnan(tot_slips))\n",
    "    )\n",
    "    dates_as_sec_masked = np.ma.masked_array(\n",
    "        dates_as_sec, mask=np.invert(~np.isnan(tot_slips))\n",
    "    )\n",
    "    tot_slips_masked = np.ma.compressed(tot_slips_masked)\n",
    "    dates_as_sec_masked = np.ma.compressed(dates_as_sec_masked)\n",
    "    tot_slips_detrended = scipy.signal.detrend(tot_slips_masked)\n",
    "\n",
    "    # Spectrogram of rolling avg slips using lomb scargle\n",
    "    frequency, power = LombScargle(dates_as_sec_masked, tot_slips_detrended).autopower()\n",
    "\n",
    "    pks = scipy.signal.find_peaks(power[:10000], height=0.06)[0]\n",
    "    # ax.plot(frequency[pks], power[pks], 'ro')\n",
    "    period = 1 / frequency / SEC_TO_DAY  # Days\n",
    "    ax3.plot(period, power)\n",
    "    ax3.set_xlabel(\"Period [Days]\", fontsize=22)\n",
    "    ax3.set_ylabel(\"Normalized Power\", fontsize=22)\n",
    "    ax3.xaxis.set_tick_params(labelsize=18)\n",
    "    ax3.yaxis.set_tick_params(labelsize=18)\n",
    "    ax3.set_xlim(0, xlim)\n",
    "    # ax.set_ylim(0,0.1)\n",
    "\n",
    "    for pk, pwr in zip(period[pks], power[pks]):\n",
    "        if pk < xlim:\n",
    "            if pk > 0:\n",
    "                print(f\"{pk} Days, Power = {pwr}\")\n",
    "                ax3.text(pk + 0.001e-5, pwr, f\"{pk:.2f} Days\", fontsize=18)\n",
    "            else:\n",
    "                print(f\"{pk} Hours, Power = {pwr}\")\n",
    "                ax3.text(pk + 0.001e-5, pwr, f\"{pk:.2f} Hours\", fontsize=18)\n",
    "\n",
    "    ax1.annotate(\"a.\", (-0.08, 0.95), xycoords=\"axes fraction\", fontsize=25)\n",
    "    ax2.annotate(\"b.\", (-0.08, 0.95), xycoords=\"axes fraction\", fontsize=25)\n",
    "    ax3.annotate(\"c.\", (-0.08, 0.95), xycoords=\"axes fraction\", fontsize=25)\n",
    "\n",
    "    st3 = datetime.datetime(2014, 1, 1)\n",
    "    ed3 = datetime.datetime(2015, 6, 1)\n",
    "\n",
    "    # Adds patch and zoom effect\n",
    "    rect = plt.Rectangle(\n",
    "        (st3, -200),\n",
    "        ed3 - st3,\n",
    "        1000,\n",
    "        facecolor=\"none\",\n",
    "        zorder=0,\n",
    "        alpha=0.3,\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=2,\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "    ax1.add_patch(rect)\n",
    "    zoom_effect01(\n",
    "        ax1,\n",
    "        ax2,\n",
    "        mdates.date2num(st3),\n",
    "        mdates.date2num(ed3),\n",
    "        ec=\"black\",\n",
    "        lw=2,\n",
    "        alpha=0.3,\n",
    "        ls=\"--\",\n",
    "    )\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f\"Rolling_Avg_{window}_Day_Window.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0fe9305b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting function\n",
    "def plot_rolling_avg_agu(\n",
    "    ax1,\n",
    "    window,\n",
    "    slide_len,\n",
    "    skip_starts,\n",
    "    skip_ends,\n",
    "    skip_middles,\n",
    "    skips,\n",
    "    tot_slips,\n",
    "    spacings,\n",
    "):\n",
    "    # Skipped slips by number of slips in xx period\n",
    "    ax2 = ax1.twinx()\n",
    "    (\n",
    "        ax1.plot(\n",
    "            dates_timeseries, ev_data[\"tidesgz05\"], zorder=1, color=\"gray\", alpha=0.4\n",
    "        ),\n",
    "    )\n",
    "    for start, interval in zip(no_data[\"starts\"][:], no_data[\"interval\"][:]):\n",
    "        start = datetime.datetime.strptime(start, \"%Y-%m-%d %H:%M:%S\")\n",
    "        rect = plt.Rectangle(\n",
    "            (start, -200), interval, 1000, facecolor=\"black\", alpha=0.1, zorder=3\n",
    "        )\n",
    "        ax1.add_patch(rect)\n",
    "\n",
    "    ax1.set_xlim(datetime.datetime(2008, 1, 1), datetime.datetime(2019, 6, 1))\n",
    "    ax1.set_ylim(-120, 130)\n",
    "\n",
    "    xerr = [(end - start) // 2 for start, end in zip(skip_starts, skip_ends)]\n",
    "    norm_slips = [i / window for i in tot_slips]\n",
    "    ax2.errorbar(\n",
    "        skip_middles,\n",
    "        norm_slips,\n",
    "        xerr=xerr,\n",
    "        fmt=\".\",\n",
    "        color=\"#21314D\",\n",
    "        zorder=2,\n",
    "        markersize=20,\n",
    "    )\n",
    "    # ax2.plot(skip_middles, tot_slips, color='blue')\n",
    "\n",
    "    timestamps = [a.timestamp() for a in skip_middles]\n",
    "\n",
    "    compressed_tot_slips = np.ma.masked_invalid(tot_slips)\n",
    "    compressed_timestamps = np.ma.masked_where(\n",
    "        np.ma.getmask(compressed_tot_slips), timestamps\n",
    "    )\n",
    "    compressed_tot_slips = np.ma.compressed(compressed_tot_slips)\n",
    "    compressed_timestamps = np.ma.compressed(compressed_timestamps)\n",
    "\n",
    "    coef = np.polyfit(compressed_timestamps, compressed_tot_slips, 1)\n",
    "    lst_sqrs_fit = np.poly1d(coef)\n",
    "\n",
    "    datetimes = [datetime.datetime.fromtimestamp(a) for a in compressed_timestamps]\n",
    "    # ax2.plot(datetimes, lst_sqrs_fit(compressed_timestamps),color='blue',zorder=3)\n",
    "\n",
    "    # Just a line so manually extract slope\n",
    "    delta_y = (\n",
    "        lst_sqrs_fit(compressed_timestamps)[-1] - lst_sqrs_fit(compressed_timestamps)[0]\n",
    "    )\n",
    "    delta_x = datetimes[-1] - datetimes[0]\n",
    "\n",
    "    print(delta_y, delta_x)\n",
    "    slope = delta_y / delta_x.total_seconds() * 3600 * 24 * 365\n",
    "    print(slope)  # Slips / year\n",
    "    ax1.yaxis.set_label_position(\"right\")\n",
    "    ax1.yaxis.tick_right()\n",
    "    ax2.yaxis.set_label_position(\"left\")\n",
    "    ax2.yaxis.tick_left()\n",
    "    ax1.set_xlabel(\"Date [yyyy]\", size=55)\n",
    "    ax1.set_ylabel(\"Tide Height [cm]\", size=55)\n",
    "    ax2.set_ylabel(f\"Slips/Day [{window} Day Window]\", size=40, color=\"black\")\n",
    "    ax1.xaxis.set_tick_params(labelsize=42)\n",
    "    ax1.yaxis.set_tick_params(labelsize=42, color=\"gray\")\n",
    "    ax2.yaxis.set_tick_params(labelsize=42, colors=\"black\")\n",
    "    # ax2.spines['left'].set_color('red')\n",
    "\n",
    "    ssa_time = datetime.timedelta(days=182.63)\n",
    "    ssa_start = datetime.datetime(2014, 3, 1)\n",
    "    annotationagu(ax1, ssa_start, ssa_start + ssa_time, 92, 96, \"$S_{sa}$\")\n",
    "\n",
    "    mm_time = datetime.timedelta(days=27.55)\n",
    "    mm_start = datetime.datetime(2013, 6, 11)\n",
    "    annotationagu(ax1, mm_start, mm_start + mm_time, 94, 98, \"$M_{m}$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96500b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotationagu(ax, x1, x2, y1, y2, text):\n",
    "    \"\"\"Draws an annotated bracket.\"\"\"\n",
    "    xs = [x1, x1, x2, x2]\n",
    "    ys = [y1, y2, y2, y1]\n",
    "    ax.plot(xs, ys, \"k-\", linewidth=3)\n",
    "    ax.text((x2 - x1) / 2 + x1, y2 + 0.1, text, ha=\"center\", va=\"bottom\", fontsize=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7732e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling avg plot for AGU24\n",
    "fig, ax1 = plt.subplots(\n",
    "    1,\n",
    "    1,\n",
    "    figsize=[90, 10],\n",
    ")\n",
    "plot_rolling_avg_agu(\n",
    "    ax1,\n",
    "    window,\n",
    "    slide,\n",
    "    skip_starts,\n",
    "    skip_ends,\n",
    "    skip_middles,\n",
    "    skips,\n",
    "    tot_slips,\n",
    "    spacings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336eb04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST OF STRAIN RATe\n",
    "import numpy as np\n",
    "from scipy.integrate import solve_ivp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parameters\n",
    "E = 1.0  # Elastic modulus (arbitrary units)\n",
    "eta = 1.0  # Viscosity (arbitrary units)\n",
    "strain_rate = 1.0  # Constant strain rate (arbitrary units)\n",
    "k = E / eta  # Parameter for nonlinear term\n",
    "sigma0 = 0.0  # Initial stress\n",
    "\n",
    "\n",
    "# Differential equation: dsigma/dt = E * strain_rate - (E/eta) * sigma^3\n",
    "def maxwell_model(t, sigma):\n",
    "    C = E * strain_rate  # Constant term from strain rate\n",
    "    return C - k * sigma**3\n",
    "\n",
    "\n",
    "# Time range for the integration\n",
    "t_span = (0, 10)  # From t=0 to t=10\n",
    "t_eval = np.linspace(t_span[0], t_span[1], 500)  # Points to evaluate\n",
    "\n",
    "# Initial condition\n",
    "y0 = [sigma0]\n",
    "\n",
    "# Solve the differential equation\n",
    "solution = solve_ivp(maxwell_model, t_span, y0, t_eval=t_eval, method=\"RK45\")\n",
    "\n",
    "# Extract results\n",
    "t = solution.t\n",
    "sigma = solution.y[0]\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(t, sigma, label=\"Stress $\\sigma(t)$\")\n",
    "plt.axhline(\n",
    "    (eta * strain_rate / E) ** (1 / 3),\n",
    "    color=\"r\",\n",
    "    linestyle=\"--\",\n",
    "    label=\"Steady-state $\\sigma_s$\",\n",
    ")\n",
    "plt.title(\"Stress Evolution in Maxwell Material\")\n",
    "plt.xlabel(\"Time $t$\")\n",
    "plt.ylabel(\"Stress $\\sigma(t)$\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "735d36a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(file):\n",
    "    \"\"\"\n",
    "    Load processed gps data file into pandas table.\n",
    "    Columns\n",
    "        longitude, latitude, elevation, time, day_of_year\n",
    "        x, y, dist\n",
    "\n",
    "    data_table = load(file)\n",
    "    \"\"\"\n",
    "    data = pd.DataFrame()  # Create Pandas DataFrame\n",
    "    flip = False\n",
    "    # Read data file\n",
    "\n",
    "    # Convert longitude and latitude from deg min sec to fractional degrees\n",
    "    # Three different file formats so try one first and try the other if it\n",
    "    # throws a not found exception.\n",
    "    try:\n",
    "        # CSRS-PPP 2024\n",
    "        d = pd.read_csv(\n",
    "            file, skiprows=3, sep=\"\\\\s+\"\n",
    "        )  # delim_whitespace=True depreciated, used \\\\s+ instead\n",
    "        # Only take 15 or 30 sec intervals if data is finer spaced\n",
    "        d = d.loc[\n",
    "            d[\"HR:MN:SS.SS\"].str.endswith(\"00.00\")\n",
    "            | d[\"HR:MN:SS.SS\"].str.endswith(\"15.00\")\n",
    "            | d[\"HR:MN:SS.SS\"].str.endswith(\"30.00\")\n",
    "            | d[\"HR:MN:SS.SS\"].str.endswith(\"45.00\")\n",
    "            | d[\"HR:MN:SS.SS\"].str.endswith(\"16.00\")  # Slw1 only, move to 15 sec\n",
    "            | d[\"HR:MN:SS.SS\"].str.endswith(\"46.00\")  # Slw1 only, move to 45 sec\n",
    "        ]\n",
    "        data[\"longitude\"] = d[\"LONDD\"] - d[\"LONMN\"] / 60 - d[\"LONSS\"] / 60 / 60\n",
    "        data[\"latitude\"] = d[\"LATDD\"] - d[\"LATMN\"] / 60 - d[\"LATSS\"] / 60 / 60\n",
    "        data[\"time\"] = pd.to_datetime(d[\"YEAR-MM-DD\"] + \"T\" + d[\"HR:MN:SS.SS\"])\n",
    "        data.loc[data[\"time\"].dt.second == 46, \"time\"] = data[\"time\"] - pd.Timedelta(\n",
    "            seconds=1\n",
    "        )  # Slw1 only, move to 45 sec\n",
    "        data.loc[data[\"time\"].dt.second == 16, \"time\"] = data[\"time\"] - pd.Timedelta(\n",
    "            seconds=1\n",
    "        )  # Slw1 only, move to 15 sec\n",
    "        data[\"day_of_year\"] = d[\"DAYofYEAR\"]\n",
    "\n",
    "    # If not in 2024 format, may be in an older format, tested for below...\n",
    "    except Exception:\n",
    "        try:\n",
    "            d = pd.read_csv(file, skiprows=7, sep=\"\\\\s+\")\n",
    "            data[\"longitude\"] = d[\"LON(d)\"] - d[\"LON(m)\"] / 60 - d[\"LON(s)\"] / 60 / 60\n",
    "            data[\"latitude\"] = d[\"LAT(d)\"] - d[\"LAT(m)\"] / 60 - d[\"LAT(s)\"] / 60 / 60\n",
    "            data[\"time\"] = pd.to_datetime(d[\"YEAR-MM-DD\"] + \"T\" + d[\"HR:MN:SS.SSS\"])\n",
    "            data[\"day_of_year\"] = d[\"DOY\"]\n",
    "        except Exception:\n",
    "            d = pd.read_csv(file, skiprows=3, sep=\"\\\\s+\")\n",
    "            data[\"longitude\"] = d[\"LONDD\"] - d[\"LONMN\"] / 60 - d[\"LONSS\"] / 60 / 60\n",
    "            data[\"latitude\"] = d[\"LATDD\"] - d[\"LATMN\"] / 60 - d[\"LATSS\"] / 60 / 60\n",
    "            data[\"time\"] = data[\"time\"] = pd.to_datetime(\n",
    "                d[\"YEAR-MM-DD\"] + \"T\" + d[\"HR:MN:SS.SSS\"]\n",
    "            )\n",
    "            data[\"day_of_year\"] = d[\"DAYofYEAR\"]\n",
    "\n",
    "    data[\"elevation\"] = d[\"HGT(m)\"]\n",
    "    data[\"sats\"] = d[\"NSV\"]\n",
    "    data[\"GDOP\"] = d[\"GDOP\"]\n",
    "\n",
    "    # Check length before\n",
    "    # Convert to Antarctic Polar Stereographic\n",
    "    x, y = ll2xy(lon=data[\"longitude\"], lat=data[\"latitude\"])\n",
    "    data[\"x\"] = x\n",
    "    data[\"y\"] = y\n",
    "\n",
    "    # Look at data and decide if to flip\n",
    "    if len(data.index) > 1:\n",
    "        diff = data[\"time\"].iloc[0] - data[\"time\"].iloc[1]\n",
    "        if diff > datetime.timedelta(seconds=0):\n",
    "            flip = True\n",
    "\n",
    "        # Keep in if statement because fails if no data\n",
    "        x0 = data[\"x\"].iloc[0]\n",
    "        y0 = data[\"y\"].iloc[0]\n",
    "\n",
    "        data[\"dist\"] = np.sqrt((data[\"x\"] - x0) ** 2 + (data[\"y\"] - y0) ** 2)\n",
    "    return data, flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece1cd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gz05, gz20 data [~3.5 min]\n",
    "# Create Catalog\n",
    "sys.path.insert(\n",
    "    0,\n",
    "    \"/mnt/c/Users/ZacharyKatz/Desktop/WhillansCatPaper/WhillansCatalogPaper/src/Catalog\",\n",
    ")\n",
    "import os\n",
    "\n",
    "import scipy.signal\n",
    "from astropy.timeseries import LombScargle\n",
    "import datetime\n",
    "\n",
    "gz05dir2011 = (\n",
    "    \"/mnt/c/users/ZacharyKatz/Desktop/Mines/FA23/GPGN598A/Term Paper/S1/gz05-2011\"\n",
    ")\n",
    "gz05dir2013 = (\n",
    "    \"/mnt/c/users/ZacharyKatz/Desktop/Mines/FA23/GPGN598A/Term Paper/S1/gz05-2013\"\n",
    ")\n",
    "gz20dir2017 = (\n",
    "    \"/mnt/c/users/ZacharyKatz/Desktop/Mines/FA23/GPGN598A/Term Paper/S1/gz20-2017\"\n",
    ")\n",
    "gz20dir2018 = (\n",
    "    \"/mnt/c/users/ZacharyKatz/Desktop/Mines/FA23/GPGN598A/Term Paper/S1/gz20-2018\"\n",
    ")\n",
    "\n",
    "# gz05 Data\n",
    "# Load time series into Dataframe\n",
    "gz052011data = pd.DataFrame()\n",
    "gz052013data = pd.DataFrame()\n",
    "gz202017data = pd.DataFrame()\n",
    "gz202018data = pd.DataFrame()\n",
    "\n",
    "dirs = [gz05dir2011, gz05dir2013, gz20dir2017, gz20dir2018]\n",
    "datas = [gz052011data, gz052013data, gz202017data, gz202018data]\n",
    "\n",
    "i = 0\n",
    "\n",
    "gz_secs = []\n",
    "gz_xs = []\n",
    "xfs = []\n",
    "tfs = []\n",
    "data_all = []\n",
    "\n",
    "for dir, data in zip(dirs, datas):\n",
    "    i = 0\n",
    "    for gps in os.listdir(dir):\n",
    "        if gps.endswith(\".pos\"):\n",
    "            i = i + 1\n",
    "            ind_data, flip = load(dir + \"/\" + gps)\n",
    "            if flip:\n",
    "                ind_data = ind_data.reindex(index=ind_data.index[::-1])\n",
    "            if i == 1 or ind_data[\"time\"].iloc[0] - data[\"time\"].iloc[\n",
    "                -1\n",
    "            ] == datetime.timedelta(seconds=15):\n",
    "                data = pd.concat([data, ind_data])\n",
    "            else:\n",
    "                print(gps)\n",
    "                break\n",
    "\n",
    "    data_all.append(data)\n",
    "\n",
    "    # Extract times as datetime objects\n",
    "    times = [\n",
    "        datetime.datetime(\n",
    "            time.year, time.month, time.day, time.hour, time.minute, time.second\n",
    "        )\n",
    "        for time in data[\"time\"]\n",
    "    ]\n",
    "\n",
    "    # Calcluate date since Jan 1, 1992 [For Comparison with PyTMD]\n",
    "    time_since_date = [\n",
    "        (time - datetime.datetime(2011, 1, 12, 0, 0, 0)) for time in times\n",
    "    ]\n",
    "    in_hours = [time.total_seconds() / 3600 for time in time_since_date]\n",
    "    in_secs = [time.total_seconds() for time in time_since_date]\n",
    "\n",
    "    # Make Spectrum\n",
    "    N = len(in_secs)\n",
    "    T = 15.0  # Time between samples\n",
    "    t = np.linspace(0.0, N * T, N, endpoint=False)\n",
    "    detrended_x = scipy.signal.detrend(data[\"elevation\"])\n",
    "    hann = scipy.signal.windows.hann(N)\n",
    "    hanned_x = detrended_x * hann  # Hann filter before Fourier Transform\n",
    "\n",
    "    xf = scipy.fft.fft(detrended_x)\n",
    "    tf = scipy.fft.fftfreq(N, T)[: N // 2]\n",
    "\n",
    "    gz_secs.append(in_secs)\n",
    "    gz_xs.append(detrended_x)\n",
    "    xfs.append(xf)\n",
    "    tfs.append(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d7a947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distance between gz05 and gz20\n",
    "dist = np.sqrt(\n",
    "    (data_all[2][\"x\"].iloc[0] - data_all[0][\"x\"].iloc[0]) ** 2\n",
    "    + (data_all[2][\"y\"].iloc[0] - data_all[0][\"y\"].iloc[0]) ** 2\n",
    ")\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cff1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in data_all:\n",
    "    print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1002812d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEC_TO_HR = 3600\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "labels = [\"GZ05 2011\", \"GZ05 2013\", \"GZ20 2017\", \"GZ20 2018\"]\n",
    "colors = [\"#440154\", \"#365c8d\", \"#1fa187\", \"#a0da39\"]\n",
    "for i, (xf, tf, gz_sec, gz_x) in enumerate(zip(xfs, tfs, gz_secs, gz_xs)):\n",
    "    N = len(gz_sec)\n",
    "    # PLot Spectrogram\n",
    "    period = 1 / tf / SEC_TO_HR  # Day\n",
    "    ax.plot(period, 12.0 / N * np.abs(xf[0 : N // 2]), label=labels[i], color=colors[i])\n",
    "ax.set_xlim(10, 28)\n",
    "ax.set_xlabel(\"Period [Hours]\", fontsize=22)\n",
    "ax.set_ylabel(\"Spectra [m/Hz]\", fontsize=22)\n",
    "ax.xaxis.set_tick_params(labelsize=18)\n",
    "ax.yaxis.set_tick_params(labelsize=18)\n",
    "ax.legend(fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0321218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find peaks for form factor analysis\n",
    "constitutents = {\"K1\": [], \"O1\": [], \"M2\": [], \"S2\": []}\n",
    "for i, (xf, tf, gz_sec, gz_x) in enumerate(zip(xfs, tfs, gz_secs, gz_xs)):\n",
    "    N = len(gz_sec)\n",
    "    # PLot Spectrogram\n",
    "    period = 1 / tf / SEC_TO_HR  # Day\n",
    "    spectra = 12.0 / N * np.abs(xf[0 : N // 2])\n",
    "    peaks = scipy.signal.find_peaks(spectra, height=0.5, distance=1)\n",
    "    for index, val in zip(peaks[0], peaks[1][\"peak_heights\"]):\n",
    "        print(f\"Period: {period[index]:.2f} Hours, Spectra: {val:.2f} m/Hz\")\n",
    "        if period[index] > 23.8 and period[index] < 24:\n",
    "            constitutents[\"K1\"].append(val)\n",
    "        elif period[index] > 25.8 and period[index] < 26:\n",
    "            constitutents[\"O1\"].append(val)\n",
    "        elif period[index] > 11.9 and period[index] < 12.1:\n",
    "            constitutents[\"M2\"].append(val)\n",
    "        elif period[index] > 12.3 and period[index] < 12.5:\n",
    "            constitutents[\"S2\"].append(val)\n",
    "\n",
    "print(constitutents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc8851f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_value = [\n",
    "    (constitutents[\"K1\"][i] + constitutents[\"O1\"][i])\n",
    "    / (constitutents[\"M2\"][i] + constitutents[\"S2\"][i])\n",
    "    for i in range(len(constitutents[\"K1\"]))\n",
    "]\n",
    "print(f_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305b6403",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEC_TO_HR = 3600\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "labels = [\n",
    "    f\"GZ05 2011, Form Factor={f_value[0]:.2f}\",\n",
    "    f\"GZ05 2013, Form Factor={f_value[1]:.2f}\",\n",
    "    f\"GZ20 2017, Form Factor={f_value[2]:.2f}\",\n",
    "    f\"GZ20 2018, Form Factor={f_value[3]:.2f}\",\n",
    "]\n",
    "colors = [\"#440154\", \"#365c8d\", \"#1fa187\", \"#a0da39\"]\n",
    "for i, (xf, tf, gz_sec, gz_x) in enumerate(zip(xfs, tfs, gz_secs, gz_xs)):\n",
    "    N = len(gz_sec)\n",
    "    # PLot Spectrogram\n",
    "    period = 1 / tf / SEC_TO_HR  # Day\n",
    "    ax.plot(period, 12.0 / N * np.abs(xf[0 : N // 2]), label=labels[i], color=colors[i])\n",
    "ax.set_xlim(10, 28)\n",
    "ax.set_xlabel(\"Period [Hours]\", fontsize=22)\n",
    "ax.set_ylabel(\"Spectra [m/Hz]\", fontsize=22)\n",
    "ax.xaxis.set_tick_params(labelsize=18)\n",
    "ax.yaxis.set_tick_params(labelsize=18)\n",
    "ax.legend(fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28dabb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all gzs and pick out peaks to compute change in tidal form factor\n",
    "SEC_TO_HR = 3600\n",
    "periods = []\n",
    "powers = []\n",
    "for i, (in_secs, detrended_x) in enumerate(zip(gz_secs, gz_xs)):\n",
    "    frequency, power = LombScargle(in_secs, detrended_x).autopower()\n",
    "    period = 1 / frequency / SEC_TO_HR  # Day\n",
    "    periods.append(period)\n",
    "    powers.append(power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1897aaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(\n",
    "    1,\n",
    "    1,\n",
    "    figsize=[18, 10],\n",
    ")\n",
    "labels = [\"GZ05 2013\", \"GZ20 2017\", \"GZ02 2018\"]\n",
    "colors = [\"#440154\", \"#365c8d\", \"#1fa187\", \"#a0da39\"]\n",
    "for i, (period, power) in enumerate(zip(periods, powers)):\n",
    "    ax.plot(period, power, color=colors[i], linewidth=3, label=labels[i])\n",
    "\n",
    "ax.set_xlabel(\"Period [Hours]\", fontsize=22)\n",
    "ax.set_ylabel(\"Normalized Power\", fontsize=22)\n",
    "ax.xaxis.set_tick_params(labelsize=18)\n",
    "ax.yaxis.set_tick_params(labelsize=18)\n",
    "ax.set_xlim(22, 28)\n",
    "ax.legend(fontsize=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6d8de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tide and rolling sum spectra as one plot\n",
    "# Lomb Scargle\n",
    "fig, ax3 = plt.subplots(\n",
    "    1,\n",
    "    1,\n",
    "    figsize=[18, 10],\n",
    ")\n",
    "dates_as_sec = [a.timestamp() for a in skip_middles]\n",
    "tot_slips_masked = np.ma.masked_array(tot_slips, mask=np.invert(~np.isnan(tot_slips)))\n",
    "dates_as_sec_masked = np.ma.masked_array(\n",
    "    dates_as_sec, mask=np.invert(~np.isnan(tot_slips))\n",
    ")\n",
    "tot_slips_masked = np.ma.compressed(tot_slips_masked)\n",
    "dates_as_sec_masked = np.ma.compressed(dates_as_sec_masked)\n",
    "tot_slips_detrended = scipy.signal.detrend(tot_slips_masked)\n",
    "\n",
    "# Spectrogram of rolling avg slips using lomb scargle\n",
    "frequency, power = LombScargle(dates_as_sec_masked, tot_slips_detrended).autopower()\n",
    "\n",
    "pks = scipy.signal.find_peaks(power[:10000], height=0.06)[0]\n",
    "# ax.plot(frequency[pks], power[pks], 'ro')\n",
    "period = 1 / frequency / SEC_TO_DAY  # Days\n",
    "ax3.plot(period, power, linewidth=3, label=\"Slip Sum Spectrum\", color=\"#21314D\")\n",
    "ax3.set_xlabel(\"Period [Days]\", fontsize=45)\n",
    "ax3.set_ylabel(\"Normalized Power\", fontsize=45)\n",
    "ax3.xaxis.set_tick_params(labelsize=40)\n",
    "ax3.yaxis.set_tick_params(labelsize=40)\n",
    "\n",
    "\n",
    "xlim = 300\n",
    "ax3.set_xlim(0.2, xlim)\n",
    "# Set log\n",
    "ax3.set_xscale(\"log\")\n",
    "# ax.set_ylim(0,0.1)\n",
    "\n",
    "frequency, power = LombScargle(in_secs, detrended_x).autopower()\n",
    "period = 1 / frequency / SEC_TO_DAY  # Day\n",
    "ax3.plot(period, power, color=\"#D34B29\", linewidth=3, label=\"GZ05 Tidal Spectrum\")\n",
    "ax3.legend(fontsize=40)\n",
    "\n",
    "\"\"\"\n",
    "for pk, pwr in zip(period[pks], power[pks]):\n",
    "    if pk < xlim:\n",
    "        if pk > 0:\n",
    "            print(f\"{pk} Days, Power = {pwr}\")\n",
    "            ax3.text(pk + 0.001e-5, pwr, f\"{pk:.2f} Days\", fontsize=18)\n",
    "        else:\n",
    "            print(f\"{pk} Hours, Power = {pwr}\")\n",
    "            ax3.text(pk + 0.001e-5, pwr, f\"{pk:.2f} Hours\", fontsize=18)\n",
    "\n",
    "\n",
    "\n",
    "# PLot Spectrogram\n",
    "HR_TO_S = 3600\n",
    "period = 1/tf / SEC_TO_DAY # Convert to hrs\n",
    "ax2.plot(period, 12.0/N * np.abs(xf[0:N//2]),'k')\n",
    "ax2.set_xlabel(\"Period [Hours]\",fontsize=14)\n",
    "ax2.set_ylabel(\"Spectrum [m/Hz]\",fontsize=14)\n",
    "#ax2.set_xlim(10,28)\n",
    "ax2.tick_params(labelsize=14)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
